% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{halevy2023will}
A.~Halevy, Y.~Choi, A.~Floratou, M.~J. Franklin, N.~Noy, and H.~Wang, ``Will llms reshape, supercharge, or kill data science?(vldb 2023 panel),'' \emph{Proceedings of the VLDB Endowment}, vol.~16, no.~12, pp. 4114--4115, 2023.

\bibitem{nascimento2023gpt}
N.~Nascimento, C.~Tavares, P.~Alencar, and D.~Cowan, ``Gpt in data science: A practical exploration of model selection,'' in \emph{2023 IEEE International Conference on Big Data (BigData)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2023, pp. 4325--4334.

\bibitem{li2024can}
J.~Li, B.~Hui, G.~Qu, J.~Yang, B.~Li, B.~Li, B.~Wang, B.~Qin, R.~Geng, N.~Huo \emph{et~al.}, ``Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls,'' \emph{Advances in Neural Information Processing Systems}, vol.~36, 2024.

\bibitem{lai2023ds}
Y.~Lai, C.~Li, Y.~Wang, T.~Zhang, R.~Zhong, L.~Zettlemoyer, W.-t. Yih, D.~Fried, S.~Wang, and T.~Yu, ``Ds-1000: A natural and reliable benchmark for data science code generation,'' in \emph{International Conference on Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2023, pp. 18\,319--18\,345.

\bibitem{kazemitabaar2024improving}
M.~Kazemitabaar, J.~Williams, I.~Drosos, T.~Grossman, A.~Z. Henley, C.~Negreanu, and A.~Sarkar, ``Improving steering and verification in ai-assisted data analysis with interactive task decomposition,'' in \emph{Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology}, 2024, pp. 1--19.

\bibitem{nguyen2022empirical}
N.~Nguyen and S.~Nadi, ``An empirical evaluation of github copilot's code suggestions,'' in \emph{Proceedings of the 19th International Conference on Mining Software Repositories}, 2022, pp. 1--5.

\bibitem{nathalia2023artificial}
N.~Nathalia, A.~Paulo, and C.~Donald, ``Artificial intelligence vs. software engineers: An empirical study on performance and efficiency using chatgpt,'' in \emph{Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering}, 2023, pp. 24--33.

\bibitem{kuhail2024will}
M.~A. Kuhail, S.~S. Mathew, A.~Khalil, J.~Berengueres, and S.~J.~H. Shah, ``“will i be replaced?” assessing chatgpt's effect on software development and programmer perceptions of ai tools,'' \emph{Science of Computer Programming}, vol. 235, p. 103111, 2024.

\bibitem{coignion2024performance}
T.~Coignion, C.~Quinton, and R.~Rouvoy, ``A performance study of llm-generated code on leetcode,'' in \emph{Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering}, 2024, pp. 79--89.

\bibitem{chen2021evaluating}
M.~Chen, J.~Tworek, H.~Jun, Q.~Yuan, H.~P. d.~O. Pinto, J.~Kaplan, H.~Edwards, Y.~Burda, N.~Joseph, G.~Brockman \emph{et~al.}, ``Evaluating large language models trained on code,'' \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem{grewal2024analyzing}
B.~Grewal, W.~Lu, S.~Nadi, and C.-P. Bezemer, ``Analyzing developer use of chatgpt generated code in open source github projects,'' in \emph{2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2024, pp. 157--161.

\bibitem{gu2024effectiveness}
X.~Gu, M.~Chen, Y.~Lin, Y.~Hu, H.~Zhang, C.~Wan, Z.~Wei, Y.~Xu, and J.~Wang, ``On the effectiveness of large language models in domain-specific code generation,'' \emph{ACM Transactions on Software Engineering and Methodology}, 2024.

\bibitem{wohlin2012experimentation}
C.~Wohlin, P.~Runeson, M.~H{\"o}st, M.~C. Ohlsson, B.~Regnell, A.~Wessl{\'e}n \emph{et~al.}, \emph{Experimentation in software engineering}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2012, vol. 236.

\bibitem{stratascratch_master_coding}
{StrataScratch}, ``Master coding for data science,'' \url{https://www.stratascratch.com/}, n.d., accessed: 2024-11-01.

\bibitem{troy2023enabling}
C.~Troy, S.~Sturley, J.~M. Alcaraz-Calero, and Q.~Wang, ``Enabling generative ai to produce sql statements: A framework for the auto-generation of knowledge based on ebnf context-free grammars,'' \emph{IEEE Access}, vol.~11, pp. 123\,543--123\,564, 2023.

\bibitem{malekpour2024towards}
M.~Malekpour, N.~Shaheen, F.~Khomh, and A.~Mhedhbi, ``Towards optimizing sql generation via llm routing,'' in \emph{NeurIPS 2024 Third Table Representation Learning Workshop}.

\bibitem{EASER_LLM4Code_Study4_2023}
\BIBentryALTinterwordspacing
S.~A. Boominathan, S.~S. Chintakunta, N.~Nascimento, and E.~Guimaraes, ``{LLM4DS-Benchmark: A Dataset for Assessing LLM Performance in Data Science Coding Tasks},'' Nov. 2024. [Online]. Available: \url{https://doi.org/10.5281/zenodo.14064111}
\BIBentrySTDinterwordspacing

\bibitem{white2023chatgpt}
J.~White, S.~Hays, Q.~Fu, J.~Spencer-Smith, and D.~C. Schmidt, ``Chatgpt prompt patterns for improving code quality, refactoring, requirements elicitation, and software design,'' 2023.

\bibitem{nascimento2023gptloop}
N.~Nascimento, P.~Alencar, and D.~Cowan, ``Gpt-in-the-loop: Supporting adaptation in multiagent systems,'' in \emph{2023 IEEE International Conference on Big Data (BigData)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2023, pp. 4674--4683.

\bibitem{polars2023}
\BIBentryALTinterwordspacing
{Ritchie Vink}, ``Polars: Blazingly fast dataframes in rust, python, node.js, r, and sql,'' 2023. [Online]. Available: \url{https://github.com/pola-rs/polars}
\BIBentrySTDinterwordspacing

\bibitem{aher2023using}
G.~V. Aher, R.~I. Arriaga, and A.~T. Kalai, ``Using large language models to simulate multiple humans and replicate human subject studies,'' in \emph{International Conference on Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2023, pp. 337--371.

\end{thebibliography}
