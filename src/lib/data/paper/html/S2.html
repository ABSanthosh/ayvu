<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>II Related Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation</title>
<!--Generated by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->

<link rel="stylesheet" href="LaTeXML.css" type="text/css">
<link rel="stylesheet" href="ltx-article.css" type="text/css">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js?config=MML_HTMLorMML"></script>
<link rel="up" href="paper.html" title="LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="start" href="paper.html" title="LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="prev" href="S1.html" title="I Introduction ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="next" href="S3.html" title="III Controlled Experiment ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S1.html" title="I Introduction ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S3.html" title="III Controlled Experiment ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S4.html" title="IV Experiment Operation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S5.html" title="V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S6.html" title="VI Discussion ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S7.html" title="VII Conclusion ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S8.html" title="VIII Future Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="bibliography" href="bib.html" title="References ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
</head>
<body>
<nav class="ltx_page_navbar"><a href="paper.html" title="" class="ltx_ref" rel="start"><span class="ltx_text ltx_ref_title">LLM4DS: Evaluating Large Language Models for Data Science Code Generation</span></a>
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a href="S1.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section ltx_ref_self"><span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></span></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S3.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Controlled Experiment</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S4.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiment Operation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S5.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Analysis and Interpretation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S6.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S7.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S8.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII </span><span class="ltx_text ltx_font_smallcaps">Future Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_bibliography"><a href="bib.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title">References</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<header class="ltx_page_header">
<div class="ltx_align_center">
<a href="paper.html" title="" class="ltx_ref" rel="up"><span class="ltx_text ltx_ref_title">LLM4DS: Evaluating Large Language Models for Data Science Code Generation</span></a><a href="S1.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref" rel="prev"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a><a href="S3.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref" rel="next"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Controlled Experiment</span></span></a>
</div></header>
<div class="ltx_page_content">
<section class="ltx_section ltx_authors_1line">
<h1 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span>
</h1>

<div id="p1" class="ltx_para">
<p class="ltx_p">In the realm of code generation, prior studies have evaluated LLMs like ChatGPT and GitHub Copilot using platforms such as HumanEval Benchmark, LeetCode, and Github. Nascimento et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="bib.html#bib21" title="Artificial intelligence vs. software engineers: an empirical study on performance and efficiency using chatgpt" class="ltx_ref">15</a>]</cite> compared code generated by ChatGPT against human-written solutions, assessing performance and memory efficiency. Kuhail et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="bib.html#bib26" title="“Will i be replaced?” assessing chatgpt’s effect on software development and programmer perceptions of ai tools" class="ltx_ref">9</a>]</cite> evaluated ChatGPT on 180 LeetCode problems, providing insights into its capabilities and limitations. Coignion et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="bib.html#bib25" title="A performance study of llm-generated code on leetcode" class="ltx_ref">4</a>]</cite> investigated different LLMs on general coding problems from LeetCode, focusing on performance metrics. Nguyen and Nadi <cite class="ltx_cite ltx_citemacro_cite">[<a href="bib.html#bib51" title="An empirical evaluation of github copilot’s code suggestions" class="ltx_ref">16</a>]</cite> assessed GitHub Copilot’s code generation on 33 LeetCode problems, evaluating correctness and understandability.</p>
</div>
<div id="p2" class="ltx_para">
<p class="ltx_p">Beyond traditional programming tasks, LLMs have been applied in data science-specific domains, where recent research has explored the models’ capacity to handle complex queries and data manipulation tasks. Troy et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="bib.html#bib12" title="Enabling generative ai to produce sql statements: a framework for the auto-generation of knowledge based on ebnf context-free grammars" class="ltx_ref">19</a>]</cite> demonstrated that LLMs could generate SQL statements for cybersecurity applications, specifically highlighting their capability in structured query generation. In another study, Malekpour et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="bib.html#bib27" title="Towards optimizing sql generation via llm routing" class="ltx_ref">12</a>]</cite> introduced an LLM routing framework designed for text-to-SQL tasks, optimizing the selection of models based on cost-efficiency and accuracy. Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="bib.html#bib66" title="Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls" class="ltx_ref">11</a>]</cite> identified limitations even in advanced models like GPT-4, noting that these models achieved only 54.89% execution accuracy on complex text-to-SQL queries—significantly below the human benchmark of 92.96%. Additionally, Kazemitabaar et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="bib.html#bib69" title="Improving steering and verification in ai-assisted data analysis with interactive task decomposition" class="ltx_ref">8</a>]</cite> delved into the challenges of data analysis with conversational AI tools like ChatGPT, identifying difficulties users face in verifying and guiding AI-generated results for desired outcomes.</p>
</div>
<div id="p3" class="ltx_para">
<p class="ltx_p">Lai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="bib.html#bib72" title="DS-1000: a natural and reliable benchmark for data science code generation" class="ltx_ref">10</a>]</cite> proposed the DS-1000 benchmark, a dataset specifically crafted for evaluating code generation in data science contexts. DS-1000 comprises 451 unique data science problems sourced from StackOverflow and spans seven essential Python libraries, including Numpy and Pandas. A key feature of this benchmark is its emphasis on problem perturbations, aimed at reducing the risk of model memorization. The dataset accounts for the unique challenges of data science tasks, which often lack executable contexts, may depend on external libraries, and can have multiple correct solutions. Lai et al. demonstrated the effect of different types of problem perturbations by testing models like Codex, InCoder, and CodeGen, with the best accuracy being 43.3% achieved by Codex-002. However, while DS-1000 provides a robust dataset for testing, Lai et al. do not perform a comparative empirical evaluation across multiple LLMs, leaving open questions about how current models fare on this benchmark.</p>
</div>
<div id="p4" class="ltx_para">
<p class="ltx_p">Despite these advancements, much of the current research has been limited to either general coding tasks or SQL-specific applications. The nuances of data science problems—ranging from data manipulation and complex analyses to visualization—remain underexplored in LLM evaluations. Our work addresses this gap by conducting an empirical experiment using four leading LLMs on a set of data science problems extracted from the Stratacratch dataset, encompassing various difficulty levels and problem types. Unlike prior studies, which primarily introduce benchmarks or focus on specific task categories, our approach offers a detailed examination of LLM performance across a broader spectrum of data science challenges.</p>
</div>
</section>
</div>
<footer class="ltx_page_footer">
<div class="ltx_align_center">
<a href="S1.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref" rel="prev"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a><a href="bib.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref" rel="bibliography"><span class="ltx_text ltx_ref_title">References</span></a><a href="S3.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref" rel="next"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Controlled Experiment</span></span></a>
</div>
<div class="ltx_page_logo">Generated  by <a href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>
</body>
</html>
