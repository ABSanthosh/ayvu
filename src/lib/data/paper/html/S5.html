<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation</title>
<!--Generated by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->

<link rel="stylesheet" href="LaTeXML.css" type="text/css">
<link rel="stylesheet" href="ltx-article.css" type="text/css">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js?config=MML_HTMLorMML"></script>
<link rel="up" href="paper.html" title="LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="start" href="paper.html" title="LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="prev" href="S4.html" title="IV Experiment Operation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="next" href="S6.html" title="VI Discussion ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S1.html" title="I Introduction ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S2.html" title="II Related Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S3.html" title="III Controlled Experiment ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S4.html" title="IV Experiment Operation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S6.html" title="VI Discussion ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S7.html" title="VII Conclusion ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S8.html" title="VIII Future Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="bibliography" href="bib.html" title="References ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
</head>
<body>
<nav class="ltx_page_navbar"><a href="paper.html" title="" class="ltx_ref" rel="start"><span class="ltx_text ltx_ref_title">LLM4DS: Evaluating Large Language Models for Data Science Code Generation</span></a>
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a href="S1.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S2.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S3.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Controlled Experiment</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S4.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiment Operation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section ltx_ref_self">
<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Analysis and Interpretation</span></span></span>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#SS1" title="In V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-A </span><span class="ltx_text ltx_font_italic">RQ1: Success Rate of LLMs in Solving Data Science Problems</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#SS2" title="In V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-B </span><span class="ltx_text ltx_font_italic">RQ2: Does the difficulty level of coding problems (easy, medium, hard) influence the success rate of the different LLMs?</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#SS3" title="In V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-C </span><span class="ltx_text ltx_font_italic">RQ3: Does the type of data science task (Analytical, Algorithm, Visualization) influence the success rate of the different LLMs?</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#SS4" title="In V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-D </span><span class="ltx_text ltx_font_italic">RQ4: For Analytical questions, do the LLMs differ in the efficiency (running time) of the code they generate?</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#SS5" title="In V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-E </span><span class="ltx_text ltx_font_italic">RQ5: For visualization tasks, do the LLMs differ in the quality (similarity) of the visual outputs they produce compared to expected results?</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a href="#SS6" title="In V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-F </span><span class="ltx_text ltx_font_italic">Threats to Validity</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#SS6.SSS1" title="In V-F Threats to Validity ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-F1 </span>Internal Validity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#SS6.SSS2" title="In V-F Threats to Validity ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-F2 </span>External Validity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#SS6.SSS3" title="In V-F Threats to Validity ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-F3 </span>Construct Validity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#SS6.SSS4" title="In V-F Threats to Validity ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-F4 </span>Conclusion Validity</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S6.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S7.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S8.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII </span><span class="ltx_text ltx_font_smallcaps">Future Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_bibliography"><a href="bib.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title">References</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<header class="ltx_page_header">
<div class="ltx_align_center">
<a href="paper.html" title="" class="ltx_ref" rel="up"><span class="ltx_text ltx_ref_title">LLM4DS: Evaluating Large Language Models for Data Science Code Generation</span></a><a href="S4.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref" rel="prev"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiment Operation</span></span></a><a href="S6.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref" rel="next"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a>
</div></header>
<div class="ltx_page_content">
<section class="ltx_section ltx_authors_1line">
<h1 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps">Analysis and Interpretation</span>
</h1>

<div id="p1" class="ltx_para">
<p class="ltx_p">This section presents the statistical analysis of data collected during the experiment. The dataset includes information such as problem IDs, the code generated by each LLM, and associated performance metrics, which is available in full for reproducibility in <cite class="ltx_cite ltx_citemacro_cite">[<a href="bib.html#bib76" title="LLM4DS-Benchmark: A Dataset for Assessing LLM Performance in Data Science Coding Tasks" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="p2" class="ltx_para">
<p class="ltx_p">Figure <a href="#F4" title="Figure 4 ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> provides a general overview of the LLMs’ assertiveness across all tasks and difficulty levels. This initial visualization offers a preliminary look at overall trends, while more detailed analyses follow for each research question (RQ).</p>
</div>
<figure id="F4" class="ltx_figure"><img src="x4.png" id="F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="789" height="621" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Overall Success Rate of LLMs.</figcaption>
</figure>
<div id="p3" class="ltx_para">
<p class="ltx_p">For each research question (RQ), we begin by visualizing the data to provide an intuitive understanding of the performance distributions across different conditions. In addition to visualization and descriptive statistics, we perform hypothesis testing for each RQ.</p>
</div>
<section id="SS1" class="ltx_subsection">
<h2 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-A </span><span class="ltx_text ltx_font_italic">RQ1: Success Rate of LLMs in Solving Data Science Problems</span>
</h2>

<figure id="F5" class="ltx_figure"><img src="x5.png" id="F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="747" height="445" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>RQ1 - LLM success rate in solving DS coding problems.</figcaption>
</figure>
<div id="SS1.p1" class="ltx_para">
<p class="ltx_p">As shown in Figure <a href="#F5" title="Figure 5 ‣ V-A RQ1: Success Rate of LLMs in Solving Data Science Problems ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, ChatGPT achieves the highest success rate (72%), followed by Claude (70%) and Perplexity (66%), with Copilot at 60%. These percentages represent the proportion of correct solutions generated by each LLM, including those needing minor code edits.</p>
</div>
<div id="SS1.p2" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Hypothesis Testing</span>: To assess each model’s success rate, we conducted a one-tailed binomial test with baseline thresholds of 50%, 60%, and 70%, determining if each LLM’s success rate significantly exceeded these benchmarks. This non-parametric test, suitable for binary outcomes (correct/incorrect), provides insight into each LLM’s performance relative to random chance <cite class="ltx_cite ltx_citemacro_cite">[<a href="bib.html#bib24" title="Experimentation in software engineering" class="ltx_ref">21</a>]</cite>. Additionally, we evaluated whether there was a significant difference in success rates between the LLMs by applying the Friedman test, followed by pairwise Wilcoxon tests where a significant difference was detected.</p>
</div>
<figure id="T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>RQ1: Success rate results of LLMs at different baselines</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:70%;">Baseline</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:70%;">LLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:70%;">Success Rate (%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:70%;">p-value</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:70%;">Conclusion</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="4"><span class="ltx_text" style="font-size:70%;">50%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Copilot</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">60%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">0.0284</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Significant</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">ChatGPT</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">72%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">0.0000</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Significant</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Perplexity</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">66%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">0.0009</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Significant</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Claude</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">70%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">0.0000</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Significant</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="4"><span class="ltx_text" style="font-size:70%;">60%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Copilot</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">60%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">0.5433</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Not Significant</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">ChatGPT</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">72%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">0.0084</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Significant</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Perplexity</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">66%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">0.1303</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Not Significant</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Claude</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">70%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">0.0248</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Significant</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" rowspan="4"><span class="ltx_text" style="font-size:70%;">70%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Copilot</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">60%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">0.9875</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Not Significant</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">ChatGPT</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">72%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">0.3768</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Not Significant</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Perplexity</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">66%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">0.8371</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Not Significant</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Claude</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">70%</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">0.5491</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Not Significant</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="SS1.p3" class="ltx_para">
<p class="ltx_p">As Table <a href="#T2" title="TABLE II ‣ V-A RQ1: Success Rate of LLMs in Solving Data Science Problems ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> shows, all LLMs perform significantly above the 50% threshold, confirming baseline effectiveness in solving coding tasks. At the 60% baseline, only ChatGPT and Claude reach statistical significance, suggesting enhanced reliability for typical tasks. No LLM achieves significance at the 70% baseline, indicating limitations in sustaining very high success rates across diverse challenges.</p>
</div>
<figure id="F6" class="ltx_figure"><img src="x6.png" id="F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="623" height="509" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>RQ1 - Pairwise Comparison of Success Rates.</figcaption>
</figure>
<div id="SS1.p4" class="ltx_para">
<p class="ltx_p">To explore differences between LLMs, we applied the Friedman test, which detected significant variation in success rates across models (p = 0.0384). We followed up with post-hoc Wilcoxon pairwise comparisons, identifying a statistically significant difference between ChatGPT and Copilot, with ChatGPT achieving a significantly higher success rate (corrected p-value: 0.0437), as depicted in the heatmap of Figure <a href="#F6" title="Figure 6 ‣ V-A RQ1: Success Rate of LLMs in Solving Data Science Problems ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. No other significant differences were observed among models.</p>
</div>
<div id="SS1.p5" class="ltx_para">
<p class="ltx_p">Based on these tests, we conclude:</p>
<blockquote class="ltx_quote">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">For hypotheses <math id="SS1.p5.m1" class="ltx_Math" alttext="H0_{1}" display="inline"><mrow><mi>H</mi><mo>⁢</mo><msub><mn>0</mn><mn>1</mn></msub></mrow></math> and <math id="SS1.p5.m2" class="ltx_Math" alttext="H0_{1a}" display="inline"><mrow><mi>H</mi><mo>⁢</mo><msub><mn>0</mn><mrow><mn>1</mn><mo>⁢</mo><mi>a</mi></mrow></msub></mrow></math>:</span></p>
<ul id="I1" class="ltx_itemize">
<li id="I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="I1.i1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">At the </span><span class="ltx_text ltx_font_bold ltx_font_italic">50% baseline</span><span class="ltx_text ltx_font_italic">, all LLMs exhibit success rates significantly above 50%, supporting the conclusion that each model performs better than random chance in solving data science coding problems.</span></p>
</div>
</li>
<li id="I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="I1.i2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">At the </span><span class="ltx_text ltx_font_bold ltx_font_italic">60% baseline</span><span class="ltx_text ltx_font_italic">, only ChatGPT and Claude show success rates significantly above this level, indicating that these two models exhibit greater reliability across general coding tasks.</span></p>
</div>
</li>
<li id="I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="I1.i3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">At the </span><span class="ltx_text ltx_font_bold ltx_font_italic">70% baseline</span><span class="ltx_text ltx_font_italic">, no LLM meets statistical significance, suggesting a possible limitation in achieving consistently high success rates across diverse coding challenges.</span></p>
</div>
</li>
<li id="I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="I1.i4.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold ltx_font_italic">Friedman Test and Wilcoxon Post-hoc Test:</span><span class="ltx_text ltx_font_italic"> Significant differences were found between models, with ChatGPT achieving a success rate significantly higher than that of Copilot.</span></p>
</div>
</li>
</ul>
</blockquote>
</div>
<div id="SS1.p6" class="ltx_para">
<p class="ltx_p">In summary, RQ1 indicates that ChatGPT and Claude exhibit the most consistent performance, particularly ChatGPT, which leads in relative success. These findings suggest that ChatGPT and Claude may be preferable for tasks demanding higher success rates, while highlighting the difficulty for LLMs in consistently achieving a 70% success rate across diverse challenges.</p>
</div>
</section>
<section id="SS2" class="ltx_subsection">
<h2 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-B </span><span class="ltx_text ltx_font_italic">RQ2: Does the difficulty level of coding problems (easy, medium, hard) influence the success rate of the different LLMs?</span>
</h2>

<figure id="F7" class="ltx_figure"><img src="x7.png" id="F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="789" height="469" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>RQ2: Effect of difficulty level on success rate.</figcaption>
</figure>
<div id="SS2.p1" class="ltx_para">
<p class="ltx_p">As shown in Figure <a href="#F7" title="Figure 7 ‣ V-B RQ2: Does the difficulty level of coding problems (easy, medium, hard) influence the success rate of the different LLMs? ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, the success rates of each LLM vary across different difficulty levels. Claude achieves the highest success rate on easy and medium problems, while ChatGPT excels on hard problems, suggesting its robustness with advanced challenges. Copilot consistently shows the lowest success rate across all difficulty levels, indicating a potential limitation in handling more complex tasks.</p>
</div>
<div id="SS2.p2" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Hypothesis Testing</span>: Chi-Square tests were performed to evaluate the effect of difficulty level on each LLM’s success rate. Results show that difficulty level significantly impacts the success rates of Perplexity and Claude (<math id="SS2.p2.m1" class="ltx_Math" alttext="p&lt;0.05" display="inline"><mrow><mi>p</mi><mo>&lt;</mo><mn>0.05</mn></mrow></math>), suggesting that their performance fluctuates with problem complexity. In contrast, Copilot and ChatGPT demonstrate consistent success rates across all difficulty levels, indicated by non-significant results.</p>
</div>
<div id="SS2.p3" class="ltx_para">
<p class="ltx_p">Based on these tests, we conclude:</p>
<blockquote class="ltx_quote">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">For the hypothesis H0_2, we reject it for Perplexity and Claude, indicating that difficulty level significantly affects their success rates. For Copilot and ChatGPT, we fail to reject H0_2, suggesting consistent performance across varying difficulty levels.</span></p>
</blockquote>
</div>
<div id="SS2.p4" class="ltx_para">
<p class="ltx_p">To further explore comparative performance, we conducted the Friedman test across all models at each difficulty level. Although the overall test did not show significant differences in success rates across LLMs for each level, pairwise Wilcoxon tests highlighted a significant difference between ChatGPT and Copilot for hard problems (p = 0.0196), indicating ChatGPT’s superior performance on more challenging tasks.</p>
</div>
</section>
<section id="SS3" class="ltx_subsection">
<h2 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-C </span><span class="ltx_text ltx_font_italic">RQ3: Does the type of data science task (Analytical, Algorithm, Visualization) influence the success rate of the different LLMs?</span>
</h2>

<figure id="F8" class="ltx_figure"><img src="x8.png" id="F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="789" height="469" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>RQ3: Effect of task type on success rate.</figcaption>
</figure>
<div id="SS3.p1" class="ltx_para">
<p class="ltx_p">Figure <a href="#F8" title="Figure 8 ‣ V-C RQ3: Does the type of data science task (Analytical, Algorithm, Visualization) influence the success rate of the different LLMs? ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> illustrates the success rate of each LLM across different task types. ChatGPT demonstrates the highest success rate in analytical and algorithm tasks, while Perplexity and Claude achieve similar levels in visualization tasks. Although ChatGPT performs particularly well in analytical and algorithm tasks, statistical tests reveal no significant overall success rate differences among the models across task types, except between ChatGPT and Copilot.</p>
</div>
<figure id="T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>RQ3: Chi-Square test results for task type across LLMs</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:70%;">LLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:70%;">Algorithm</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:70%;">Analytical</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:70%;">Visuali.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:70%;">p-value</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:70%;">Conclusion</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Copilot</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">22/35</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">17/35</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">21/30</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">0.1946</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Not Sig.</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">ChatGPT</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">26/35</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">25/35</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">21/30</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">0.9250</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Not Sig.</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Perplexity</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">23/35</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">20/35</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">23/30</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">0.2534</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Not Sig.</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Claude</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">26/35</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">21/35</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">23/30</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">0.2715</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:70%;">Not Sig.</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="SS3.p2" class="ltx_para">
<p class="ltx_p">The Chi-Square test results in Table <a href="#T3" title="TABLE III ‣ V-C RQ3: Does the type of data science task (Analytical, Algorithm, Visualization) influence the success rate of the different LLMs? ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> show that task type does not significantly impact the success rate for any LLM, with all p-values exceeding the 0.05 threshold. This finding suggests that each model’s performance remains relatively stable across analytical, algorithmic, and visualization tasks.</p>
</div>
<div id="SS3.p3" class="ltx_para">
<blockquote class="ltx_quote">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">For hypothesis H0_3, we fail to reject it for all models, indicating that task type does not significantly impact success rate overall. However, post-hoc comparisons reveal that ChatGPT performs significantly better than Copilot in analytical and algorithm tasks.</span></p>
</blockquote>
</div>
</section>
<section id="SS4" class="ltx_subsection">
<h2 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-D </span><span class="ltx_text ltx_font_italic">RQ4: For Analytical questions, do the LLMs differ in the efficiency (running time) of the code they generate?</span>
</h2>

<figure id="F9" class="ltx_figure"><img src="x9.png" id="F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="831" height="480" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>RQ4: Execution times of LLMs - Box Plot.</figcaption>
</figure>
<figure id="F10" class="ltx_figure"><img src="x10.png" id="F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="748" height="479" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>RQ4: Median execution time by difficulty level.</figcaption>
</figure>
<div id="SS4.p1" class="ltx_para">
<p class="ltx_p">For a fair comparison, Figures <a href="#F9" title="Figure 9 ‣ V-D RQ4: For Analytical questions, do the LLMs differ in the efficiency (running time) of the code they generate? ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> and <a href="#F10" title="Figure 10 ‣ V-D RQ4: For Analytical questions, do the LLMs differ in the efficiency (running time) of the code they generate? ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> include only the results from problems successfully solved by all LLMs, as the platform does not compute execution times for solutions that did not work. Accordingly, Claude has the lowest median execution time, indicating its solutions generally execute faster than the other models’ solutions, followed by Copilot and Perplexity. ChatGPT has the highest median execution time, suggesting that on average, it takes longer to execute analytical tasks than the other models. ChatGPT displays the largest interquartile range (IQR), indicating significant variability, whereas Copilot, Perplexity, and Claude have narrower IQRs, suggesting more consistent execution times. This analysis suggests Claude is generally faster and more consistent for analytical tasks, while ChatGPT may offer less predictability in execution time.</p>
</div>
<div id="SS4.p2" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Hypothesis Testing</span>: To assess whether these observed differences are statistically significant, we conducted a Kruskal-Wallis test, as the Kruskal-Wallis test is a non-parametric method suitable for comparing the distributions of independent groups, particularly their central tendencies, when data is not normally distributed.</p>
</div>
<div id="SS4.p3" class="ltx_para">
<p class="ltx_p">The test, conducted using the <span class="ltx_text ltx_font_typewriter">scipy.stats</span> library, resulted in a Kruskal-Wallis statistic of 0.6947 and a p-value of 0.8744. With a p-value exceeding the significance level of 0.05, we fail to reject the null hypothesis. This suggests that there are no statistically significant differences in the median execution times across the LLMs for Analytical questions.</p>
</div>
<div id="SS4.p4" class="ltx_para">
<blockquote class="ltx_quote">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">For RQ4, we fail to reject H0_4, indicating that the LLMs do not differ significantly in the efficiency (running time) of the code they generate for Analytical questions.</span></p>
</blockquote>
</div>
</section>
<section id="SS5" class="ltx_subsection">
<h2 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-E </span><span class="ltx_text ltx_font_italic">RQ5: For visualization tasks, do the LLMs differ in the quality (similarity) of the visual outputs they produce compared to expected results?</span>
</h2>

<figure id="F11" class="ltx_figure"><img src="x11.png" id="F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="831" height="487" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>RQ5: Similarity Scores - Box Plot.</figcaption>
</figure>
<figure id="F12" class="ltx_figure"><img src="x12.png" id="F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="830" height="528" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>RQ5: Median similarity scores by difficulty level.</figcaption>
</figure>
<div id="SS5.p1" class="ltx_para">
<p class="ltx_p">As depicted in Figures <a href="#F11" title="Figure 11 ‣ V-E RQ5: For visualization tasks, do the LLMs differ in the quality (similarity) of the visual outputs they produce compared to expected results? ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> and <a href="#F12" title="Figure 12 ‣ V-E RQ5: For visualization tasks, do the LLMs differ in the quality (similarity) of the visual outputs they produce compared to expected results? ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>, ChatGPT achieves the highest median similarity score among the commonly solved problems, indicating that its outputs are closest to the expected results. Additionally, ChatGPT displays the narrowest interquartile range (IQR), highlighting its consistency. These findings suggest that ChatGPT delivers more reliable quality in generating visual outputs that closely match the expected results.</p>
</div>
<div id="SS5.p2" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Hypothesis Testing</span>: To statistically analyze differences in similarity scores among the LLMs, we conducted a Kruskal-Wallis test.</p>
</div>
<div id="SS5.p3" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Kruskal-Wallis Test Results</span>:</p>
<ul id="I2" class="ltx_itemize">
<li id="I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="I2.i1.p1" class="ltx_para">
<p class="ltx_p">Kruskal-Wallis Statistic: 0.8287</p>
</div>
</li>
<li id="I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="I2.i2.p1" class="ltx_para">
<p class="ltx_p">p-value: 0.8426</p>
</div>
</li>
<li id="I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="I2.i3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Conclusion</span>: The p-value above 0.05 suggests no statistically significant differences in similarity scores between the LLMs. This indicates that while there are observed differences in mean similarity scores and variability (with ChatGPT achieving the highest mean and most consistent performance), these differences are not statistically significant across LLMs at the 5% significance level.</p>
</div>
</li>
</ul>
</div>
<div id="SS5.p4" class="ltx_para">
<p class="ltx_p">Based on these results, we conclude:</p>
<blockquote class="ltx_quote">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">For hypothesis H0_5, we fail to reject the null hypothesis, indicating that there is no significant difference in the similarity quality of generated visualization outputs among the LLMs.</span></p>
</blockquote>
</div>
</section>
<section id="SS6" class="ltx_subsection">
<h2 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-F </span><span class="ltx_text ltx_font_italic">Threats to Validity</span>
</h2>

<div id="SS6.p1" class="ltx_para">
<p class="ltx_p">As usual in empirical studies, our study acknowledges several threats that may impact the interpretation and generalization of the results.</p>
</div>
<section id="SS6.SSS1" class="ltx_subsubsection">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">V-F1 </span>Internal Validity</h3>

<div id="SS6.SSS1.p1" class="ltx_para">
<p class="ltx_p">A key concern is the undisclosed nature of the LLMs’ training data. Without access to this information, we cannot confirm whether the generated solutions are novel or based on memorized content. Even though we selected new problems from Stratascratch, similar or identical problems might exist in the models’ training data, potentially inflating their apparent effectiveness.</p>
</div>
<div id="SS6.SSS1.p2" class="ltx_para">
<p class="ltx_p">Prompt design is another factor influencing outcomes. As noted by White et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="bib.html#bib36" title="ChatGPT prompt patterns for improving code quality, refactoring, requirements elicitation, and software design" class="ltx_ref">20</a>]</cite>, the formulation of prompts can significantly affect LLM outputs. While we endeavored to use consistent prompts derived from original problem descriptions, variations could lead to different results.</p>
</div>
<div id="SS6.SSS1.p3" class="ltx_para">
<p class="ltx_p">To address potential subjectivity in converting problems to prompts, we developed standardized prompt templates for each task type. These templates ensured that all AI assistants received clear, consistent instructions, allowing for a fair comparison of performance.</p>
</div>
</section>
<section id="SS6.SSS2" class="ltx_subsubsection">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">V-F2 </span>External Validity</h3>

<div id="SS6.SSS2.p1" class="ltx_para">
<p class="ltx_p">The generalizability of our findings is limited by the scope of problems used. Our study focused on 100 Python coding problems from a single platform, which may not represent the full spectrum of data science tasks. To enhance external validity, future research should incorporate a wider range of problems from multiple sources.</p>
</div>
</section>
<section id="SS6.SSS3" class="ltx_subsubsection">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">V-F3 </span>Construct Validity</h3>

<div id="SS6.SSS3.p1" class="ltx_para">
<p class="ltx_p">We did not formally assess the expertise of the researchers conducting the experiment, which could introduce subjectivity, particularly in interpreting and evaluating the AI-generated code. Although guidelines were established for acceptable code modifications—allowing only minor edits to resolve execution issues—differences in coding proficiency among researchers could influence the assessment.</p>
</div>
</section>
<section id="SS6.SSS4" class="ltx_subsubsection">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">V-F4 </span>Conclusion Validity</h3>

<div id="SS6.SSS4.p1" class="ltx_para">
<p class="ltx_p">These threats may affect the validity of our conclusions. While our study offers insights into the capabilities and limitations of LLMs in data science code generation, the results should be interpreted with caution. Further research addressing these limitations is necessary to strengthen the confidence in the findings.</p>
</div>
</section>
</section>
</section>
</div>
<footer class="ltx_page_footer">
<div class="ltx_align_center">
<a href="S4.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref" rel="prev"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiment Operation</span></span></a><a href="bib.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref" rel="bibliography"><span class="ltx_text ltx_ref_title">References</span></a><a href="S6.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref" rel="next"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a>
</div>
<div class="ltx_page_logo">Generated  by <a href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>
</body>
</html>
