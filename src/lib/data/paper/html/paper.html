<!doctype html>
<html lang="en">
	<head>
		<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
		<title>LLM4DS: Evaluating Large Language Models for Data Science Code Generation</title>
		<!--Generated by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->

		<!-- <link rel="stylesheet" href="LaTeXML.css" type="text/css" />
		<link rel="stylesheet" href="ltx-article.css" type="text/css" /> -->
		<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
		<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js?config=MML_HTMLorMML"></script>
		<meta
			name="keywords"
			lang="en"
			content="
data science,  large language model,  coding generation,  empirical study,  hypothesis testing
"
		/>
	</head>
	<body>
		<nav class="ltx_page_navbar">
			<nav class="ltx_TOC">
				<ol class="ltx_toclist">
					<li class="ltx_tocentry ltx_tocentry_section">
						<a
							href="#S1"
							title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
							class="ltx_ref"
							><span class="ltx_text ltx_ref_title"
								><span class="ltx_tag ltx_tag_ref">I </span
								><span class="ltx_text ltx_font_smallcaps">Introduction</span></span
							></a
						>
					</li>
					<li class="ltx_tocentry ltx_tocentry_section">
						<a
							href="#S2"
							title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
							class="ltx_ref"
							><span class="ltx_text ltx_ref_title"
								><span class="ltx_tag ltx_tag_ref">II </span
								><span class="ltx_text ltx_font_smallcaps">Related Work</span></span
							></a
						>
					</li>
					<li class="ltx_tocentry ltx_tocentry_section">
						<a
							href="#S3"
							title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
							class="ltx_ref"
							><span class="ltx_text ltx_ref_title"
								><span class="ltx_tag ltx_tag_ref">III </span
								><span class="ltx_text ltx_font_smallcaps">Controlled Experiment</span></span
							></a
						>
						<ol class="ltx_toclist ltx_toclist_section">
							<li class="ltx_tocentry ltx_tocentry_subsection">
								<a
									href="#S3.SS1"
									title="In III Controlled Experiment ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
									class="ltx_ref"
									><span class="ltx_text ltx_ref_title"
										><span class="ltx_tag ltx_tag_ref">III-A </span
										><span class="ltx_text ltx_font_italic"
											>Research Questions, Hypotheses, and Metrics</span
										></span
									></a
								>
							</li>
							<li class="ltx_tocentry ltx_tocentry_subsection">
								<a
									href="#S3.SS2"
									title="In III Controlled Experiment ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
									class="ltx_ref"
									><span class="ltx_text ltx_ref_title"
										><span class="ltx_tag ltx_tag_ref">III-B </span
										><span class="ltx_text ltx_font_italic">Variables Selection</span></span
									></a
								>
							</li>
						</ol>
					</li>
					<li class="ltx_tocentry ltx_tocentry_section">
						<a
							href="#S4"
							title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
							class="ltx_ref"
							><span class="ltx_text ltx_ref_title"
								><span class="ltx_tag ltx_tag_ref">IV </span
								><span class="ltx_text ltx_font_smallcaps">Experiment Operation</span></span
							></a
						>
						<ol class="ltx_toclist ltx_toclist_section">
							<li class="ltx_tocentry ltx_tocentry_subsection">
								<a
									href="#S4.SS1"
									title="In IV Experiment Operation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
									class="ltx_ref"
									><span class="ltx_text ltx_ref_title"
										><span class="ltx_tag ltx_tag_ref">IV-A </span
										><span class="ltx_text ltx_font_italic"
											>Dataset: Selection of Data Science Problems</span
										></span
									></a
								>
							</li>
							<li class="ltx_tocentry ltx_tocentry_subsection">
								<a
									href="#S4.SS2"
									title="In IV Experiment Operation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
									class="ltx_ref"
									><span class="ltx_text ltx_ref_title"
										><span class="ltx_tag ltx_tag_ref">IV-B </span
										><span class="ltx_text ltx_font_italic"
											>Prompt Engineering: Transforming Problems into Prompts</span
										></span
									></a
								>
							</li>
							<li class="ltx_tocentry ltx_tocentry_subsection">
								<a
									href="#S4.SS3"
									title="In IV Experiment Operation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
									class="ltx_ref"
									><span class="ltx_text ltx_ref_title"
										><span class="ltx_tag ltx_tag_ref">IV-C </span
										><span class="ltx_text ltx_font_italic"
											>Code Generation and Execution</span
										></span
									></a
								>
							</li>
						</ol>
					</li>
					<li class="ltx_tocentry ltx_tocentry_section">
						<a
							href="#S5"
							title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
							class="ltx_ref"
							><span class="ltx_text ltx_ref_title"
								><span class="ltx_tag ltx_tag_ref">V </span
								><span class="ltx_text ltx_font_smallcaps">Analysis and Interpretation</span></span
							></a
						>
						<ol class="ltx_toclist ltx_toclist_section">
							<li class="ltx_tocentry ltx_tocentry_subsection">
								<a
									href="#S5.SS1"
									title="In V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
									class="ltx_ref"
									><span class="ltx_text ltx_ref_title"
										><span class="ltx_tag ltx_tag_ref">V-A </span
										><span class="ltx_text ltx_font_italic"
											>RQ1: Success Rate of LLMs in Solving Data Science Problems</span
										></span
									></a
								>
							</li>
							<li class="ltx_tocentry ltx_tocentry_subsection">
								<a
									href="#S5.SS2"
									title="In V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
									class="ltx_ref"
									><span class="ltx_text ltx_ref_title"
										><span class="ltx_tag ltx_tag_ref">V-B </span
										><span class="ltx_text ltx_font_italic"
											>RQ2: Does the difficulty level of coding problems (easy, medium, hard)
											influence the success rate of the different LLMs?</span
										></span
									></a
								>
							</li>
							<li class="ltx_tocentry ltx_tocentry_subsection">
								<a
									href="#S5.SS3"
									title="In V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
									class="ltx_ref"
									><span class="ltx_text ltx_ref_title"
										><span class="ltx_tag ltx_tag_ref">V-C </span
										><span class="ltx_text ltx_font_italic"
											>RQ3: Does the type of data science task (Analytical, Algorithm,
											Visualization) influence the success rate of the different LLMs?</span
										></span
									></a
								>
							</li>
							<li class="ltx_tocentry ltx_tocentry_subsection">
								<a
									href="#S5.SS4"
									title="In V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
									class="ltx_ref"
									><span class="ltx_text ltx_ref_title"
										><span class="ltx_tag ltx_tag_ref">V-D </span
										><span class="ltx_text ltx_font_italic"
											>RQ4: For Analytical questions, do the LLMs differ in the efficiency (running
											time) of the code they generate?</span
										></span
									></a
								>
							</li>
							<li class="ltx_tocentry ltx_tocentry_subsection">
								<a
									href="#S5.SS5"
									title="In V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
									class="ltx_ref"
									><span class="ltx_text ltx_ref_title"
										><span class="ltx_tag ltx_tag_ref">V-E </span
										><span class="ltx_text ltx_font_italic"
											>RQ5: For visualization tasks, do the LLMs differ in the quality (similarity)
											of the visual outputs they produce compared to expected results?</span
										></span
									></a
								>
							</li>
							<li class="ltx_tocentry ltx_tocentry_subsection">
								<a
									href="#S5.SS6"
									title="In V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
									class="ltx_ref"
									><span class="ltx_text ltx_ref_title"
										><span class="ltx_tag ltx_tag_ref">V-F </span
										><span class="ltx_text ltx_font_italic">Threats to Validity</span></span
									></a
								>
								<ol class="ltx_toclist ltx_toclist_subsection">
									<li class="ltx_tocentry ltx_tocentry_subsubsection">
										<a
											href="#S5.SS6.SSS1"
											title="In V-F Threats to Validity ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
											class="ltx_ref"
											><span class="ltx_text ltx_ref_title"
												><span class="ltx_tag ltx_tag_ref">V-F1 </span>Internal Validity</span
											></a
										>
									</li>
									<li class="ltx_tocentry ltx_tocentry_subsubsection">
										<a
											href="#S5.SS6.SSS2"
											title="In V-F Threats to Validity ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
											class="ltx_ref"
											><span class="ltx_text ltx_ref_title"
												><span class="ltx_tag ltx_tag_ref">V-F2 </span>External Validity</span
											></a
										>
									</li>
									<li class="ltx_tocentry ltx_tocentry_subsubsection">
										<a
											href="#S5.SS6.SSS3"
											title="In V-F Threats to Validity ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
											class="ltx_ref"
											><span class="ltx_text ltx_ref_title"
												><span class="ltx_tag ltx_tag_ref">V-F3 </span>Construct Validity</span
											></a
										>
									</li>
									<li class="ltx_tocentry ltx_tocentry_subsubsection">
										<a
											href="#S5.SS6.SSS4"
											title="In V-F Threats to Validity ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
											class="ltx_ref"
											><span class="ltx_text ltx_ref_title"
												><span class="ltx_tag ltx_tag_ref">V-F4 </span>Conclusion Validity</span
											></a
										>
									</li>
								</ol>
							</li>
						</ol>
					</li>
					<li class="ltx_tocentry ltx_tocentry_section">
						<a
							href="#S6"
							title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
							class="ltx_ref"
							><span class="ltx_text ltx_ref_title"
								><span class="ltx_tag ltx_tag_ref">VI </span
								><span class="ltx_text ltx_font_smallcaps">Discussion</span></span
							></a
						>
					</li>
					<li class="ltx_tocentry ltx_tocentry_section">
						<a
							href="#S7"
							title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
							class="ltx_ref"
							><span class="ltx_text ltx_ref_title"
								><span class="ltx_tag ltx_tag_ref">VII </span
								><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span
							></a
						>
					</li>
					<li class="ltx_tocentry ltx_tocentry_section">
						<a
							href="#S8"
							title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
							class="ltx_ref"
							><span class="ltx_text ltx_ref_title"
								><span class="ltx_tag ltx_tag_ref">VIII </span
								><span class="ltx_text ltx_font_smallcaps">Future Work</span></span
							></a
						>
						<ol class="ltx_toclist ltx_toclist_section">
							<li class="ltx_tocentry ltx_tocentry_subsection">
								<a
									href="#S8.SS1"
									title="In VIII Future Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
									class="ltx_ref"
									><span class="ltx_text ltx_ref_title"
										><span class="ltx_tag ltx_tag_ref">VIII-A </span
										><span class="ltx_text ltx_font_italic"
											>Exploring Complex and Real-World Data Science Tasks</span
										></span
									></a
								>
							</li>
							<li class="ltx_tocentry ltx_tocentry_subsection">
								<a
									href="#S8.SS2"
									title="In VIII Future Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
									class="ltx_ref"
									><span class="ltx_text ltx_ref_title"
										><span class="ltx_tag ltx_tag_ref">VIII-B </span
										><span class="ltx_text ltx_font_italic"
											>Expanding Model Diversity and Dataset Coverage</span
										></span
									></a
								>
							</li>
							<li class="ltx_tocentry ltx_tocentry_subsection">
								<a
									href="#S8.SS3"
									title="In VIII Future Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
									class="ltx_ref"
									><span class="ltx_text ltx_ref_title"
										><span class="ltx_tag ltx_tag_ref">VIII-C </span
										><span class="ltx_text ltx_font_italic"
											>Expanding Evaluation Metrics</span
										></span
									></a
								>
							</li>
							<li class="ltx_tocentry ltx_tocentry_subsection">
								<a
									href="#S8.SS4"
									title="In VIII Future Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
									class="ltx_ref"
									><span class="ltx_text ltx_ref_title"
										><span class="ltx_tag ltx_tag_ref">VIII-D </span
										><span class="ltx_text ltx_font_italic"
											>Investigating Prompt Engineering and Ensuring Reproducibility</span
										></span
									></a
								>
							</li>
							<li class="ltx_tocentry ltx_tocentry_subsection">
								<a
									href="#S8.SS5"
									title="In VIII Future Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
									class="ltx_ref"
									><span class="ltx_text ltx_ref_title"
										><span class="ltx_tag ltx_tag_ref">VIII-E </span
										><span class="ltx_text ltx_font_italic"
											>Exploring Further Research Questions</span
										></span
									></a
								>
							</li>
						</ol>
					</li>
				</ol>
			</nav>
		</nav>
		<div class="ltx_page_main">
			<div class="ltx_page_content">
				<article class="ltx_document ltx_authors_1line">
					<h1 class="ltx_title ltx_title_document">
						LLM4DS: Evaluating Large Language Models for Data Science Code Generation
					</h1>
					<div class="ltx_authors">
						<span class="ltx_creator ltx_role_author">
							<span class="ltx_personname"> Nathalia Nascimento </span
							><span class="ltx_author_notes">
								<span class="ltx_contact ltx_role_affiliation"
									><span class="ltx_text ltx_font_italic">EASER, Eng. Division</span>
									<br class="ltx_break" /><span class="ltx_text ltx_font_italic"
										>Pennsylvania State University <br class="ltx_break" /></span
									>Great Valley, USA <br class="ltx_break" />nqm5742@psu.edu
								</span></span
							></span
						>
						<span class="ltx_author_before">  </span
						><span class="ltx_creator ltx_role_author">
							<span class="ltx_personname">Everton Guimaraes </span
							><span class="ltx_author_notes">
								<span class="ltx_contact ltx_role_affiliation"
									><span class="ltx_text ltx_font_italic">EASER, Eng. Division</span>
									<br class="ltx_break" /><span class="ltx_text ltx_font_italic">
										Pennsylvania State University <br class="ltx_break" /></span
									>Great Valley, USA <br class="ltx_break" />ezt157@psu.edu
								</span></span
							></span
						>
						<span class="ltx_author_before">  </span
						><span class="ltx_creator ltx_role_author">
							<span class="ltx_personname">Sai Sanjna Chintakunta </span
							><span class="ltx_author_notes">
								<span class="ltx_contact ltx_role_affiliation"
									><span class="ltx_text ltx_font_italic">EASER, Eng. Division</span>
									<br class="ltx_break" /><span class="ltx_text ltx_font_italic">
										Pennsylvania State University <br class="ltx_break" /></span
									>Great Valley, USA <br class="ltx_break" />sqc6557@psu.edu
								</span></span
							></span
						>
						<span class="ltx_author_before">  </span
						><span class="ltx_creator ltx_role_author">
							<span class="ltx_personname">Santhosh Anitha Boominathan </span
							><span class="ltx_author_notes">
								<span class="ltx_contact ltx_role_affiliation"
									><span class="ltx_text ltx_font_italic">EASER, Eng. Division</span>
									<br class="ltx_break" /><span class="ltx_text ltx_font_italic"
										>Pennsylvania State University <br class="ltx_break" /></span
									>Great Valley, USA <br class="ltx_break" />sfa5971@psu.edu
								</span></span
							></span
						>
					</div>

					<div class="ltx_abstract">
						<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
						<p class="ltx_p">
							The adoption of Large Language Models (LLMs) for code generation in data science
							offers substantial potential for enhancing tasks such as data manipulation,
							statistical analysis, and visualization. However, the effectiveness of these models in
							the data science domain remains underexplored. This paper presents a controlled
							experiment that empirically assesses the performance of four leading LLM-based AI
							assistants—Microsoft Copilot (GPT-4 Turbo), ChatGPT (o1-preview), Claude (3.5 Sonnet),
							and Perplexity Labs (Llama-3.1-70b-instruct)—on a diverse set of data science coding
							challenges sourced from the Stratacratch platform. Using the Goal-Question-Metric
							(GQM) approach, we evaluated each model’s effectiveness across task types (Analytical,
							Algorithm, Visualization) and varying difficulty levels. Our findings reveal that all
							models exceeded a 50% baseline success rate, confirming their capability beyond random
							chance. Notably, only ChatGPT and Claude achieved success rates significantly above a
							60% baseline, though none of the models reached a 70% threshold, indicating
							limitations in higher standards. ChatGPT demonstrated consistent performance across
							varying difficulty levels, while Claude’s success rate fluctuated with task
							complexity. Hypothesis testing indicates that task type does not significantly impact
							success rate overall. For analytical tasks, efficiency analysis shows no significant
							differences in execution times, though ChatGPT tended to be slower and less
							predictable despite high success rates. For visualization tasks, while similarity
							quality among LLMs is comparable, ChatGPT consistently delivered the most accurate
							outputs. This study provides a structured, empirical evaluation of LLMs in data
							science, delivering insights that support informed model selection tailored to
							specific task demands. Our findings establish a framework for future AI assessments,
							emphasizing the value of rigorous evaluation beyond basic accuracy measures.
						</p>
					</div>
					<div class="ltx_keywords">
						<h6 class="ltx_title ltx_title_keywords">Index Terms:</h6>
						data science, large language model, coding generation, empirical study, hypothesis
						testing
					</div>
					<section id="S1" class="ltx_section">
						<h2 class="ltx_title ltx_title_section">
							<span class="ltx_tag ltx_tag_section">I </span
							><span class="ltx_text ltx_font_smallcaps">Introduction</span>
						</h2>

						<div id="S1.p1" class="ltx_para">
							<p class="ltx_p">
								Large Language Models (LLMs) have emerged as transformative tools with the potential
								to revolutionize code generation in various domains, including data science
								<cite class="ltx_cite ltx_citemacro_cite"
									>[<a
										href="#bib.bib8"
										title="Will llms reshape, supercharge, or kill data science?(vldb 2023 panel)"
										class="ltx_ref"
										>7</a
									>,
									<a
										href="#bib.bib65"
										title="GPT in data science: a practical exploration of model selection"
										class="ltx_ref"
										>14</a
									>,
									<a
										href="#bib.bib66"
										title="Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls"
										class="ltx_ref"
										>11</a
									>,
									<a
										href="#bib.bib72"
										title="DS-1000: a natural and reliable benchmark for data science code generation"
										class="ltx_ref"
										>10</a
									>,
									<a
										href="#bib.bib69"
										title="Improving steering and verification in ai-assisted data analysis with interactive task decomposition"
										class="ltx_ref"
										>8</a
									>]</cite
								>. Their ability to generate human-like text and code opens up possibilities for
								automating complex tasks in data manipulation, visualization, and analytics. As data
								science projects often require extensive coding efforts that are time-consuming and
								demand significant expertise, leveraging LLMs could greatly enhance productivity and
								accessibility in this field. However, the effectiveness and reliability of
								LLM-generated code for data science applications remain underexplored, necessitating
								a thorough evaluation.
							</p>
						</div>
						<div id="S1.p2" class="ltx_para">
							<p class="ltx_p">
								While previous studies have evaluated LLMs in general programming tasks using
								platforms like LeetCode
								<cite class="ltx_cite ltx_citemacro_cite"
									>[<a
										href="#bib.bib51"
										title="An empirical evaluation of github copilot’s code suggestions"
										class="ltx_ref"
										>16</a
									>,
									<a
										href="#bib.bib21"
										title="Artificial intelligence vs. software engineers: an empirical study on performance and efficiency using chatgpt"
										class="ltx_ref"
										>15</a
									>,
									<a
										href="#bib.bib26"
										title="“Will i be replaced?” assessing chatgpt’s effect on software development and programmer perceptions of ai tools"
										class="ltx_ref"
										>9</a
									>,
									<a
										href="#bib.bib25"
										title="A performance study of llm-generated code on leetcode"
										class="ltx_ref"
										>4</a
									>]</cite
								>, the HumanEval benchmark
								<cite class="ltx_cite ltx_citemacro_cite"
									>[<a
										href="#bib.bib53"
										title="Evaluating large language models trained on code"
										class="ltx_ref"
										>3</a
									>]</cite
								>, and GitHub Projects
								<cite class="ltx_cite ltx_citemacro_cite"
									>[<a
										href="#bib.bib74"
										title="Analyzing developer use of chatgpt generated code in open source github projects"
										class="ltx_ref"
										>5</a
									>]</cite
								>, Gu et al.
								<cite class="ltx_cite ltx_citemacro_cite"
									>[<a
										href="#bib.bib73"
										title="On the effectiveness of large language models in domain-specific code generation"
										class="ltx_ref"
										>6</a
									>]</cite
								>
								identified a notable gap in approaches to evaluate domain-specific code generation.
								They demonstrated that LLMs exhibit sub-optimal performance in generating
								domain-specific code for areas such as web and game development, due to their
								limited proficiency in utilizing domain-specific libraries. This finding underscores
								the need for more focused evaluations that consider the unique challenges of
								specialized domains like data science, which involve tasks such as handling
								datasets, performing complex statistical analyses, and generating insightful
								visualizations—areas not fully represented in general programming assessments.
							</p>
						</div>
						<div id="S1.p3" class="ltx_para">
							<p class="ltx_p">
								This paper addresses this gap by providing an empirical evaluation
								<cite class="ltx_cite ltx_citemacro_cite"
									>[<a
										href="#bib.bib24"
										title="Experimentation in software engineering"
										class="ltx_ref"
										>21</a
									>]</cite
								>
								of multiple LLMs on diverse data science-specific coding problems sourced from the
								Stratascratch platform
								<cite class="ltx_cite ltx_citemacro_cite"
									>[<a href="#bib.bib70" title="Master coding for data science" class="ltx_ref"
										>18</a
									>]</cite
								>. The controlled experiment involves four main steps: (i) selecting 100 Python
								coding problems from Stratascratch, distributed across three difficulty levels
								(easy, medium, hard) and three problem types (Analytical, Algorithm, Visualization);
								(ii) transforming these problems into prompts following the optimal prompt structure
								for each type; (iii) using these prompts for each AI assistant to generate code
								solutions; and (iv) evaluating the generated code based on correctness, efficiency,
								and other relevant metrics.
							</p>
						</div>
						<div id="S1.p4" class="ltx_para">
							<p class="ltx_p">
								Our research seeks to answer the following question: How effective are LLMs for data
								science coding? By systematically assessing the performance of these AI assistants,
								we aim to identify their strengths and limitations in automating code generation for
								data science problems.
							</p>
						</div>
						<div id="S1.p5" class="ltx_para">
							<p class="ltx_p">
								<span class="ltx_text ltx_font_bold">Our contributions are multifold:</span>
							</p>
							<ol id="S1.I1" class="ltx_enumerate">
								<li id="S1.I1.i1" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">1.</span>
									<div id="S1.I1.i1.p1" class="ltx_para">
										<p class="ltx_p">
											We provide an empirical evaluation of multiple LLMs on data science-specific
											coding problems, filling a critical gap in current research.
										</p>
									</div>
								</li>
								<li id="S1.I1.i2" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">2.</span>
									<div id="S1.I1.i2.p1" class="ltx_para">
										<p class="ltx_p">
											We assess Stratacratch as a platform to benchmark LLMs for data science code
											generation, evaluating its suitability and potential as a standardized dataset
											for LLM performance in this domain.
										</p>
									</div>
								</li>
								<li id="S1.I1.i3" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">3.</span>
									<div id="S1.I1.i3.p1" class="ltx_para">
										<p class="ltx_p">
											We analyze the success rate of these models across different task
											categories—Analytical, Algorithm, and Visualization—and difficulty levels,
											offering insights into their practical utility in data science workflows.
										</p>
									</div>
								</li>
								<li id="S1.I1.i4" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">4.</span>
									<div id="S1.I1.i4.p1" class="ltx_para">
										<p class="ltx_p">
											We highlight the challenges and limitations of LLMs in this domain, providing
											a foundation for future improvements and research in AI-assisted data science.
										</p>
									</div>
								</li>
							</ol>
						</div>
						<div id="S1.p6" class="ltx_para">
							<p class="ltx_p">
								This paper is organized as follows. Section 2 presents the related work. Section 3
								describes the controlled experiment, outlining the research questions, hypotheses,
								and methodology. Section 4-5 presents the experimental results and discusses threats
								to validity. Sections 6-8 brings final remarks and suggestions for future work.
							</p>
						</div>
					</section>
					<section id="S2" class="ltx_section">
						<h2 class="ltx_title ltx_title_section">
							<span class="ltx_tag ltx_tag_section">II </span
							><span class="ltx_text ltx_font_smallcaps">Related Work</span>
						</h2>

						<div id="S2.p1" class="ltx_para">
							<p class="ltx_p">
								In the realm of code generation, prior studies have evaluated LLMs like ChatGPT and
								GitHub Copilot using platforms such as HumanEval Benchmark, LeetCode, and Github.
								Nascimento et al.
								<cite class="ltx_cite ltx_citemacro_cite"
									>[<a
										href="#bib.bib21"
										title="Artificial intelligence vs. software engineers: an empirical study on performance and efficiency using chatgpt"
										class="ltx_ref"
										>15</a
									>]</cite
								>
								compared code generated by ChatGPT against human-written solutions, assessing
								performance and memory efficiency. Kuhail et al.
								<cite class="ltx_cite ltx_citemacro_cite"
									>[<a
										href="#bib.bib26"
										title="“Will i be replaced?” assessing chatgpt’s effect on software development and programmer perceptions of ai tools"
										class="ltx_ref"
										>9</a
									>]</cite
								>
								evaluated ChatGPT on 180 LeetCode problems, providing insights into its capabilities
								and limitations. Coignion et al.
								<cite class="ltx_cite ltx_citemacro_cite"
									>[<a
										href="#bib.bib25"
										title="A performance study of llm-generated code on leetcode"
										class="ltx_ref"
										>4</a
									>]</cite
								>
								investigated different LLMs on general coding problems from LeetCode, focusing on
								performance metrics. Nguyen and Nadi
								<cite class="ltx_cite ltx_citemacro_cite"
									>[<a
										href="#bib.bib51"
										title="An empirical evaluation of github copilot’s code suggestions"
										class="ltx_ref"
										>16</a
									>]</cite
								>
								assessed GitHub Copilot’s code generation on 33 LeetCode problems, evaluating
								correctness and understandability.
							</p>
						</div>
						<div id="S2.p2" class="ltx_para">
							<p class="ltx_p">
								Beyond traditional programming tasks, LLMs have been applied in data
								science-specific domains, where recent research has explored the models’ capacity to
								handle complex queries and data manipulation tasks. Troy et al.
								<cite class="ltx_cite ltx_citemacro_cite"
									>[<a
										href="#bib.bib12"
										title="Enabling generative ai to produce sql statements: a framework for the auto-generation of knowledge based on ebnf context-free grammars"
										class="ltx_ref"
										>19</a
									>]</cite
								>
								demonstrated that LLMs could generate SQL statements for cybersecurity applications,
								specifically highlighting their capability in structured query generation. In
								another study, Malekpour et al.
								<cite class="ltx_cite ltx_citemacro_cite"
									>[<a
										href="#bib.bib27"
										title="Towards optimizing sql generation via llm routing"
										class="ltx_ref"
										>12</a
									>]</cite
								>
								introduced an LLM routing framework designed for text-to-SQL tasks, optimizing the
								selection of models based on cost-efficiency and accuracy. Li et al.
								<cite class="ltx_cite ltx_citemacro_cite"
									>[<a
										href="#bib.bib66"
										title="Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls"
										class="ltx_ref"
										>11</a
									>]</cite
								>
								identified limitations even in advanced models like GPT-4, noting that these models
								achieved only 54.89% execution accuracy on complex text-to-SQL queries—significantly
								below the human benchmark of 92.96%. Additionally, Kazemitabaar et al.
								<cite class="ltx_cite ltx_citemacro_cite"
									>[<a
										href="#bib.bib69"
										title="Improving steering and verification in ai-assisted data analysis with interactive task decomposition"
										class="ltx_ref"
										>8</a
									>]</cite
								>
								delved into the challenges of data analysis with conversational AI tools like
								ChatGPT, identifying difficulties users face in verifying and guiding AI-generated
								results for desired outcomes.
							</p>
						</div>
						<div id="S2.p3" class="ltx_para">
							<p class="ltx_p">
								Lai et al.
								<cite class="ltx_cite ltx_citemacro_cite"
									>[<a
										href="#bib.bib72"
										title="DS-1000: a natural and reliable benchmark for data science code generation"
										class="ltx_ref"
										>10</a
									>]</cite
								>
								proposed the DS-1000 benchmark, a dataset specifically crafted for evaluating code
								generation in data science contexts. DS-1000 comprises 451 unique data science
								problems sourced from StackOverflow and spans seven essential Python libraries,
								including Numpy and Pandas. A key feature of this benchmark is its emphasis on
								problem perturbations, aimed at reducing the risk of model memorization. The dataset
								accounts for the unique challenges of data science tasks, which often lack
								executable contexts, may depend on external libraries, and can have multiple correct
								solutions. Lai et al. demonstrated the effect of different types of problem
								perturbations by testing models like Codex, InCoder, and CodeGen, with the best
								accuracy being 43.3% achieved by Codex-002. However, while DS-1000 provides a robust
								dataset for testing, Lai et al. do not perform a comparative empirical evaluation
								across multiple LLMs, leaving open questions about how current models fare on this
								benchmark.
							</p>
						</div>
						<div id="S2.p4" class="ltx_para">
							<p class="ltx_p">
								Despite these advancements, much of the current research has been limited to either
								general coding tasks or SQL-specific applications. The nuances of data science
								problems—ranging from data manipulation and complex analyses to visualization—remain
								underexplored in LLM evaluations. Our work addresses this gap by conducting an
								empirical experiment using four leading LLMs on a set of data science problems
								extracted from the Stratacratch dataset, encompassing various difficulty levels and
								problem types. Unlike prior studies, which primarily introduce benchmarks or focus
								on specific task categories, our approach offers a detailed examination of LLM
								performance across a broader spectrum of data science challenges.
							</p>
						</div>
					</section>
					<section id="S3" class="ltx_section">
						<h2 class="ltx_title ltx_title_section">
							<span class="ltx_tag ltx_tag_section">III </span
							><span class="ltx_text ltx_font_smallcaps">Controlled Experiment</span>
						</h2>

						<div id="S3.p1" class="ltx_para">
							<p class="ltx_p">
								In line with the controlled experiment methodology by Wohlin et al.
								<cite class="ltx_cite ltx_citemacro_cite"
									>[<a
										href="#bib.bib24"
										title="Experimentation in software engineering"
										class="ltx_ref"
										>21</a
									>]</cite
								>, our study aims to evaluate and compare the effectiveness of four prominent
								LLM-based AI assistants—Microsoft Copilot (GPT-4 Turbo), ChatGPT (o1-preview),
								Claude (3.5 Sonnet), and Perplexity Lab (Llama-3.1-70b-instruct)—in solving data
								science coding tasks sourced from the Stratascratch platform
								<cite class="ltx_cite ltx_citemacro_cite"
									>[<a href="#bib.bib70" title="Master coding for data science" class="ltx_ref"
										>18</a
									>]</cite
								>.
							</p>
						</div>
						<div id="S3.p2" class="ltx_para">
							<p class="ltx_p">
								<span class="ltx_text ltx_font_bold">Effectiveness</span> in this context refers to
								the degree to which these models achieve desired outcomes across four key aspects:
								<span class="ltx_text ltx_font_italic">success rate</span>,
								<span class="ltx_text ltx_font_italic">efficiency</span>,
								<span class="ltx_text ltx_font_italic">quality of output</span>, and
								<span class="ltx_text ltx_font_italic">consistency</span>. Specifically, we define:
							</p>
							<ul id="S3.I1" class="ltx_itemize">
								<li id="S3.I1.i1" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">•</span>
									<div id="S3.I1.i1.p1" class="ltx_para">
										<p class="ltx_p">
											<span class="ltx_text ltx_font_bold">Success Rate</span> as the proportion of
											correctly generated code solutions, measured by the percentage of solutions
											that achieve the correct result regardless of the number of attempts;
										</p>
									</div>
								</li>
								<li id="S3.I1.i2" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">•</span>
									<div id="S3.I1.i2.p1" class="ltx_para">
										<p class="ltx_p">
											<span class="ltx_text ltx_font_bold">Efficiency</span> as the runtime
											execution speed of the generated solution;
										</p>
									</div>
								</li>
								<li id="S3.I1.i3" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">•</span>
									<div id="S3.I1.i3.p1" class="ltx_para">
										<p class="ltx_p">
											<span class="ltx_text ltx_font_bold">Quality of Output</span> as the alignment
											of generated solutions with expected outcomes, particularly for visualization
											tasks;
										</p>
									</div>
								</li>
								<li id="S3.I1.i4" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">•</span>
									<div id="S3.I1.i4.p1" class="ltx_para">
										<p class="ltx_p">
											<span class="ltx_text ltx_font_bold">Consistency</span> as the reliability of
											each model’s performance across varying difficulty levels and task types.
										</p>
									</div>
								</li>
							</ul>
						</div>
						<section id="S3.SS1" class="ltx_subsection">
							<h3 class="ltx_title ltx_title_subsection">
								<span class="ltx_tag ltx_tag_subsection">III-A </span
								><span class="ltx_text ltx_font_italic"
									>Research Questions, Hypotheses, and Metrics</span
								>
							</h3>

							<div id="S3.SS1.p1" class="ltx_para">
								<p class="ltx_p">
									To systematically explore effectiveness, we structured our investigation around
									specific research questions, each accompanied by testable hypotheses and relevant
									evaluation metrics. Table
									<a
										href="#S3.T1"
										title="TABLE I ‣ III-A Research Questions, Hypotheses, and Metrics ‣ III Controlled Experiment ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">I</span></a
									>
									details these research questions, hypotheses, and corresponding metrics.
								</p>
							</div>
							<figure id="S3.T1" class="ltx_table">
								<figcaption class="ltx_caption ltx_centering">
									<span class="ltx_tag ltx_tag_table">TABLE I: </span>Research Questions,
									Hypotheses, and Metrics
								</figcaption>
								<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
									<thead class="ltx_thead">
										<tr class="ltx_tr">
											<th
												class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"
											>
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 148pt"
														><span class="ltx_text ltx_font_bold">Research Question</span></span
													>
												</span>
											</th>
											<th
												class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t"
											>
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 105.3pt"
														><span class="ltx_text ltx_font_bold">Null Hypothesis</span></span
													>
												</span>
											</th>
											<th
												class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t"
											>
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 105.3pt"
														><span class="ltx_text ltx_font_bold"
															>Alternative Hypothesis</span
														></span
													>
												</span>
											</th>
											<th
												class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t"
											>
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 105.3pt"
														><span class="ltx_text ltx_font_bold">Metrics</span></span
													>
												</span>
											</th>
										</tr>
									</thead>
									<tbody class="ltx_tbody">
										<tr class="ltx_tr">
											<td
												class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t"
											>
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 148pt"
														>RQ1: How successful are LLMs in solving data science coding problems,
														and do they outperform each other in success rate?</span
													>
												</span>
											</td>
											<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 105.3pt"
														>H0_1: The success rate of each LLM in solving data science coding
														problems is not significantly higher than random chance (50%).
														<br class="ltx_break" />H0_1a: There is no significant difference in
														success rates between LLM pairs.</span
													>
												</span>
											</td>
											<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 105.3pt"
														>H1_1: The success rate of each LLM in solving data science coding
														problems is significantly higher than random chance (50%).
														<br class="ltx_break" />H1_1a: At least one pair of LLMs shows a
														significant difference in success rates.</span
													>
												</span>
											</td>
											<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 105.3pt"
														>Overall success rate (percentage of correct solutions) and pairwise
														success rate comparisons</span
													>
												</span>
											</td>
										</tr>
										<tr class="ltx_tr">
											<td
												class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t"
											>
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 148pt"
														>RQ2: Does the difficulty level of coding problems (easy, medium, hard)
														influence the success rate of the different LLMs, and do specific LLMs
														outperform others at each difficulty level?</span
													>
												</span>
											</td>
											<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 105.3pt"
														>H0_2: Difficulty level does not significantly affect the success rate
														of the LLMs. <br class="ltx_break" />H0_2a: There is no significant
														difference in success rates between LLM pairs within each difficulty
														level.</span
													>
												</span>
											</td>
											<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 105.3pt"
														>H1_2: The success rate of the LLMs varies significantly with difficulty
														level. <br class="ltx_break" />H1_2a: At least one pair of LLMs shows a
														significant difference in success rate within a specific difficulty
														level.</span
													>
												</span>
											</td>
											<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 105.3pt"
														>Success rate (percentage of correct solutions) across difficulty levels
														and pairwise success rate comparisons within each level</span
													>
												</span>
											</td>
										</tr>
										<tr class="ltx_tr">
											<td
												class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t"
											>
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 148pt"
														>RQ3: Does the type of data science task (Analytical, Algorithm,
														Visualization) influence the success rate of the different LLMs, and do
														specific LLMs outperform others for certain task types?</span
													>
												</span>
											</td>
											<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 105.3pt"
														>H0_3: The type of data science task does not significantly impact the
														LLMs’ success rate. <br class="ltx_break" />H0_3a: There is no
														significant difference in success rates between LLM pairs within each
														task type.</span
													>
												</span>
											</td>
											<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 105.3pt"
														>H1_3: The success rate of the LLMs varies significantly with the type
														of data science task. <br class="ltx_break" />H1_3a: At least one pair
														of LLMs shows a significant difference in success rate within a specific
														task type.</span
													>
												</span>
											</td>
											<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 105.3pt"
														>Success rate (percentage of correct solutions) for each task type and
														pairwise success rate comparisons within each type</span
													>
												</span>
											</td>
										</tr>
										<tr class="ltx_tr">
											<td
												class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t"
											>
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 148pt"
														>RQ4: For Analytical questions, do the LLMs differ in the efficiency
														(execution time) of the code they generate?</span
													>
												</span>
											</td>
											<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 105.3pt"
														>H0_4: The population medians of the execution times across the LLMs for
														Analytical questions are equal.</span
													>
												</span>
											</td>
											<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 105.3pt"
														>H1_4: At least one LLM has a different population median execution time
														for Analytical questions compared to others.</span
													>
												</span>
											</td>
											<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 105.3pt"
														>Execution time for each generated solution on Analytical problems, per
														LLM</span
													>
												</span>
											</td>
										</tr>
										<tr class="ltx_tr">
											<td
												class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t"
											>
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 148pt"
														>RQ5: For visualization tasks, do the LLMs differ in the quality
														(similarity) of the visual outputs they produce compared to expected
														results?</span
													>
												</span>
											</td>
											<td
												class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t"
											>
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 105.3pt"
														>H0_5: The population medians of the similarity scores for visualization
														outputs across the LLMs are equal.</span
													>
												</span>
											</td>
											<td
												class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t"
											>
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 105.3pt"
														>H1_5: At least one LLM has a different population median similarity
														score for visualization outputs compared to others.</span
													>
												</span>
											</td>
											<td
												class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t"
											>
												<span class="ltx_inline-block ltx_align_top">
													<span class="ltx_p" style="width: 105.3pt"
														>Similarity scores for visualization outputs compared to expected
														results</span
													>
												</span>
											</td>
										</tr>
									</tbody>
								</table>
							</figure>
						</section>
						<section id="S3.SS2" class="ltx_subsection">
							<h3 class="ltx_title ltx_title_subsection">
								<span class="ltx_tag ltx_tag_subsection">III-B </span
								><span class="ltx_text ltx_font_italic">Variables Selection</span>
							</h3>

							<div id="S3.SS2.p1" class="ltx_para">
								<p class="ltx_p">
									To structure our analysis, we identified key variables that allow us to examine
									the performance of each AI assistant across different problem types and difficulty
									levels.
								</p>
							</div>
							<div id="S3.SS2.p2" class="ltx_para">
								<p class="ltx_p">
									The <span class="ltx_text ltx_font_bold">independent variables</span> in this
									study, which we controlled or varied, include:
								</p>
								<ul id="S3.I2" class="ltx_itemize">
									<li id="S3.I2.i1" class="ltx_item" style="list-style-type: none">
										<span class="ltx_tag ltx_tag_item">•</span>
										<div id="S3.I2.i1.p1" class="ltx_para">
											<p class="ltx_p">
												<span class="ltx_text ltx_font_bold">LLM-based AI assistants</span>: The
												four AI models under evaluation—Microsoft Copilot, ChatGPT, Claude, and
												Perplexity Lab.
											</p>
										</div>
									</li>
									<li id="S3.I2.i2" class="ltx_item" style="list-style-type: none">
										<span class="ltx_tag ltx_tag_item">•</span>
										<div id="S3.I2.i2.p1" class="ltx_para">
											<p class="ltx_p">
												<span class="ltx_text ltx_font_bold"
													>Difficulty level of coding problems</span
												>: Easy, Medium, Hard.
											</p>
										</div>
									</li>
									<li id="S3.I2.i3" class="ltx_item" style="list-style-type: none">
										<span class="ltx_tag ltx_tag_item">•</span>
										<div id="S3.I2.i3.p1" class="ltx_para">
											<p class="ltx_p">
												<span class="ltx_text ltx_font_bold">Type of Data Science task</span>:
												Analytical, Algorithm, Visualization.
											</p>
										</div>
									</li>
								</ul>
							</div>
							<div id="S3.SS2.p3" class="ltx_para">
								<p class="ltx_p">
									The <span class="ltx_text ltx_font_bold">dependent variables</span> are the
									metrics we measured to assess each AI assistant’s effectiveness:
								</p>
								<ul id="S3.I3" class="ltx_itemize">
									<li id="S3.I3.i1" class="ltx_item" style="list-style-type: none">
										<span class="ltx_tag ltx_tag_item">•</span>
										<div id="S3.I3.i1.p1" class="ltx_para">
											<p class="ltx_p">
												<span class="ltx_text ltx_font_bold">Success rate</span>: The percentage of
												correct solutions generated by each LLM, regardless of the number of
												attempts.
											</p>
										</div>
									</li>
									<li id="S3.I3.i2" class="ltx_item" style="list-style-type: none">
										<span class="ltx_tag ltx_tag_item">•</span>
										<div id="S3.I3.i2.p1" class="ltx_para">
											<p class="ltx_p">
												<span class="ltx_text ltx_font_bold">Running time</span>: Execution time of
												code for Analytical questions.
											</p>
										</div>
									</li>
									<li id="S3.I3.i3" class="ltx_item" style="list-style-type: none">
										<span class="ltx_tag ltx_tag_item">•</span>
										<div id="S3.I3.i3.p1" class="ltx_para">
											<p class="ltx_p">
												<span class="ltx_text ltx_font_bold">Graph similarity scores</span>:
												Similarity between generated and expected graphs for Visualization
												questions.
											</p>
										</div>
									</li>
								</ul>
							</div>
							<div id="S3.SS2.p4" class="ltx_para">
								<p class="ltx_p">
									These variables connect directly to the research questions and metrics outlined in
									Table
									<a
										href="#S3.T1"
										title="TABLE I ‣ III-A Research Questions, Hypotheses, and Metrics ‣ III Controlled Experiment ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">I</span></a
									>, allowing us to systematically investigate the impact of each independent
									variable on the AI models’ performance.
								</p>
							</div>
						</section>
					</section>
					<section id="S4" class="ltx_section">
						<h2 class="ltx_title ltx_title_section">
							<span class="ltx_tag ltx_tag_section">IV </span
							><span class="ltx_text ltx_font_smallcaps">Experiment Operation</span>
						</h2>

						<div id="S4.p1" class="ltx_para">
							<p class="ltx_p">
								The experiment was conducted over the span of two months. For each of the four LLMs,
								we generated a solution for each of the 100 selected problems, resulting in a total
								of 400 generated coding solutions. Two researchers manually interacted with the AI
								assistants by inputting the prompts into their respective interfaces. They then
								copied the generated code and submitted it to the Stratascratch platform to assess
								its correctness and functionality. The researchers recorded whether the solution
								worked as intended and noted any necessary adjustments. Since Stratascratch provides
								execution time only for Analytical questions and similarity scores for Visualization
								questions, we collected these specific measurements accordingly.
							</p>
						</div>
						<div id="S4.p2" class="ltx_para">
							<p class="ltx_p">
								The overall process of our controlled experiment consists of 11 steps, as
								illustrated in Figure
								<a
									href="#S4.F1"
									title="Figure 1 ‣ IV Experiment Operation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
									class="ltx_ref"
									><span class="ltx_text ltx_ref_tag">1</span></a
								>:
							</p>
						</div>
						<div id="S4.p3" class="ltx_para">
							<ol id="S4.I1" class="ltx_enumerate">
								<li id="S4.I1.i1" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">1.</span>
									<div id="S4.I1.i1.p1" class="ltx_para">
										<p class="ltx_p">
											Select the problem source: We chose Stratascratch
											<cite class="ltx_cite ltx_citemacro_cite"
												>[<a
													href="#bib.bib70"
													title="Master coding for data science"
													class="ltx_ref"
													>18</a
												>]</cite
											>
											as the platform for sourcing data science problems.
										</p>
									</div>
								</li>
								<li id="S4.I1.i2" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">2.</span>
									<div id="S4.I1.i2.p1" class="ltx_para">
										<p class="ltx_p">
											Select one problem per task category for prompt engineering: One problem from
											each data science task category (Analytical, Algorithm, Visualization) was
											selected to refine our prompt templates.
										</p>
									</div>
								</li>
								<li id="S4.I1.i3" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">3.</span>
									<div id="S4.I1.i3.p1" class="ltx_para">
										<p class="ltx_p">
											Prompt Engineering with feedback loop: We performed prompt engineering by
											iteratively adjusting the prompts and assessing the performance of different
											LLM versions, creating optimal prompt structures for each task type.
										</p>
									</div>
								</li>
								<li id="S4.I1.i4" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">4.</span>
									<div id="S4.I1.i4.p1" class="ltx_para">
										<p class="ltx_p">
											Selection of AI assistants and LLMs: Four AI assistants, each utilizing a
											different LLM, were selected for the experiment.
										</p>
									</div>
								</li>
								<li id="S4.I1.i5" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">5.</span>
									<div id="S4.I1.i5.p1" class="ltx_para">
										<p class="ltx_p">
											Definition of final prompts: The finalized prompt templates were established
											for each problem type based on the prompt engineering process.
										</p>
									</div>
								</li>
								<li id="S4.I1.i6" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">6.</span>
									<div id="S4.I1.i6.p1" class="ltx_para">
										<p class="ltx_p">
											Selection of 100 Data Science problems: We selected 100 data science problems
											covering various topics across the three task types to ensure a comprehensive
											evaluation.
										</p>
									</div>
								</li>
								<li id="S4.I1.i7" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">7.</span>
									<div id="S4.I1.i7.p1" class="ltx_para">
										<p class="ltx_p">
											Creation of prompts: The selected problems were incorporated into the prompt
											templates, resulting in 100 tailored prompts.
										</p>
									</div>
								</li>
								<li id="S4.I1.i8" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">8.</span>
									<div id="S4.I1.i8.p1" class="ltx_para">
										<p class="ltx_p">
											Execution with AI assistants: Each prompt was executed using the four AI
											assistants, and the generated Python code was saved.
										</p>
									</div>
								</li>
								<li id="S4.I1.i9" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">9.</span>
									<div id="S4.I1.i9.p1" class="ltx_para">
										<p class="ltx_p">
											Submission to Stratascratch platform: The generated code solutions were
											submitted to the Stratascratch platform interface for evaluation.
										</p>
									</div>
								</li>
								<li id="S4.I1.i10" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">10.</span>
									<div id="S4.I1.i10.p1" class="ltx_para">
										<p class="ltx_p">
											Execution and result collection: The code was executed on Stratascratch, and
											the results were saved into a results dataset.
										</p>
									</div>
								</li>
								<li id="S4.I1.i11" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">11.</span>
									<div id="S4.I1.i11.p1" class="ltx_para">
										<p class="ltx_p">
											Data analysis: We compared the performance results of the four LLMs to analyze
											their effectiveness.
										</p>
									</div>
								</li>
							</ol>
						</div>
						<figure id="S4.F1" class="ltx_figure">
							<img
								src="x1.png"
								id="S4.F1.g1"
								class="ltx_graphics ltx_centering ltx_img_portrait"
								width="747"
								height="1361"
								alt="Refer to caption"
							/>
							<figcaption class="ltx_caption ltx_centering">
								<span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of the Experimental
								Process.
							</figcaption>
						</figure>
						<div id="S4.p4" class="ltx_para">
							<p class="ltx_p">
								The following subsections and sections provide more detailed explanations of the
								main steps:
							</p>
							<ul id="S4.I2" class="ltx_itemize">
								<li id="S4.I2.i1" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">•</span>
									<div id="S4.I2.i1.p1" class="ltx_para">
										<p class="ltx_p">
											Subsection
											<a
												href="#S4.SS1"
												title="IV-A Dataset: Selection of Data Science Problems ‣ IV Experiment Operation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
												class="ltx_ref"
												><span class="ltx_text ltx_ref_tag">IV-A</span></a
											>
											describes the selection of 100 Python coding problems from Stratascratch,
											categorized by difficulty levels (easy, medium, hard) and types (Analytical,
											Algorithm, Visualization).
										</p>
									</div>
								</li>
								<li id="S4.I2.i2" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">•</span>
									<div id="S4.I2.i2.p1" class="ltx_para">
										<p class="ltx_p">
											Subsection
											<a
												href="#S4.SS2"
												title="IV-B Prompt Engineering: Transforming Problems into Prompts ‣ IV Experiment Operation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
												class="ltx_ref"
												><span class="ltx_text ltx_ref_tag">IV-B</span></a
											>
											outlines the iterative prompt engineering process, including the development
											and refinement of prompt templates for each task type. This resulted in
											optimal prompt structures used to transform the selected problems into 100
											tailored prompts.
										</p>
									</div>
								</li>
								<li id="S4.I2.i3" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">•</span>
									<div id="S4.I2.i3.p1" class="ltx_para">
										<p class="ltx_p">
											Subsection
											<a
												href="#S4.SS3"
												title="IV-C Code Generation and Execution ‣ IV Experiment Operation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
												class="ltx_ref"
												><span class="ltx_text ltx_ref_tag">IV-C</span></a
											>
											explains the process of using these prompts with each AI assistant to generate
											code solutions.
										</p>
									</div>
								</li>
								<li id="S4.I2.i4" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">•</span>
									<div id="S4.I2.i4.p1" class="ltx_para">
										<p class="ltx_p">
											Section
											<a
												href="#S5"
												title="V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
												class="ltx_ref"
												><span class="ltx_text ltx_ref_tag">V</span></a
											>
											covers the data analysis process.
										</p>
									</div>
								</li>
							</ul>
						</div>
						<section id="S4.SS1" class="ltx_subsection">
							<h3 class="ltx_title ltx_title_subsection">
								<span class="ltx_tag ltx_tag_subsection">IV-A </span
								><span class="ltx_text ltx_font_italic"
									>Dataset: Selection of Data Science Problems</span
								>
							</h3>

							<div id="S4.SS1.p1" class="ltx_para">
								<p class="ltx_p">
									For our study, we selected the Stratascratch
									<cite class="ltx_cite ltx_citemacro_cite"
										>[<a href="#bib.bib70" title="Master coding for data science" class="ltx_ref"
											>18</a
										>]</cite
									>
									platform as the source of data science coding problems. Stratascratch is a
									platform that aggregates real-world data science interview questions from various
									companies, providing a diverse set of problems that are representative of typical
									tasks encountered in data science, such as data manipulation, algorithm
									development, and data visualization.
								</p>
							</div>
							<div id="S4.SS1.p2" class="ltx_para">
								<p class="ltx_p">
									Stratascratch problems are organized into three difficulty levels (easy, medium,
									and hard) and three main types, each addressing unique aspects of data science
									problem-solving:
								</p>
							</div>
							<div id="S4.SS1.p3" class="ltx_para">
								<p class="ltx_p">
									<span class="ltx_text ltx_font_bold">Analytical:</span> These problems involve
									tasks requiring data analysis and manipulation using tools like
									<span class="ltx_text ltx_font_typewriter">pandas</span> and SQL. Topics include
									data aggregation, filtering, conditional expressions, and data formatting.
								</p>
							</div>
							<div id="S4.SS1.p4" class="ltx_para">
								<p class="ltx_p">
									<span class="ltx_text ltx_font_bold">Algorithm:</span> These challenges focus on
									computational problem-solving and algorithm development. Topics in this category
									include array manipulation, linear regression, probability, graph theory,
									recursion, and optimization techniques.
								</p>
							</div>
							<div id="S4.SS1.p5" class="ltx_para">
								<p class="ltx_p">
									<span class="ltx_text ltx_font_bold">Visualization:</span> These problems require
									the creation of charts and graphs to represent data insights visually. Topics
									cover distribution analysis, time-series trend analysis, spatial data
									visualization, and comparison of categorical and numerical data.
								</p>
							</div>
							<div id="S4.SS1.p6" class="ltx_para">
								<p class="ltx_p">
									An example Stratascratch problem is shown in Figure <a
										href="#S4.F2"
										title="Figure 2 ‣ IV-A Dataset: Selection of Data Science Problems ‣ IV Experiment Operation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">2</span></a
									>, demonstrating the typical interface and information available for each
									question.
								</p>
							</div>
							<figure id="S4.F2" class="ltx_figure">
								<p class="ltx_p ltx_align_center">
									<span
										class="ltx_text ltx_framed ltx_framed_rectangle"
										style="border-color: #000000"
										><img
											src="x2.png"
											id="S4.F2.g1"
											class="ltx_graphics ltx_img_landscape"
											width="568"
											height="361"
											alt="Refer to caption"
									/></span>
								</p>
								<figcaption class="ltx_caption ltx_centering">
									<span class="ltx_tag ltx_tag_figure">Figure 2: </span>Example of a Visualization
									Problem from the Stratascratch platform.
								</figcaption>
							</figure>
							<div id="S4.SS1.p7" class="ltx_para">
								<p class="ltx_p">
									To build our dataset, we used random sampling while ensuring balanced
									representation across problem types and difficulty levels—selecting 100 Python
									coding problems in total, with 35 Analytical, 35 Algorithm, and 30 Visualization
									problems. From these 100 questions, 34 are easy, 32 are medium, and 34 are hard.
									To avoid infringing any intellectual property from Stratascratch, we omitted the
									full problem descriptions from our dataset. However, we have provided a table
									containing problem IDs, difficulty levels, links, and topic descriptions, which
									gives sufficient context for each task (dataset available in
									<cite class="ltx_cite ltx_citemacro_cite"
										>[<a
											href="#bib.bib76"
											title="LLM4DS-Benchmark: A Dataset for Assessing LLM Performance in Data Science Coding Tasks"
											class="ltx_ref"
											>2</a
										>]</cite
									>).
								</p>
							</div>
						</section>
						<section id="S4.SS2" class="ltx_subsection">
							<h3 class="ltx_title ltx_title_subsection">
								<span class="ltx_tag ltx_tag_subsection">IV-B </span
								><span class="ltx_text ltx_font_italic"
									>Prompt Engineering: Transforming Problems into Prompts</span
								>
							</h3>

							<div id="S4.SS2.p1" class="ltx_para">
								<span class="ltx_ERROR undefined">{tcolorbox}</span>
								<p class="ltx_p">
									[title=Prompt Template for Visualization Problems, colback=white, colframe=black]
									<span class="ltx_text" style="font-size: 70%"
										>Act as a data scientist and provide working Python3 code for the problem
										below.</span
									>
								</p>
							</div>
							<div id="S4.SS2.p2" class="ltx_para">
								<p class="ltx_p">
									<span class="ltx_text ltx_font_bold" style="font-size: 70%"
										>CRITICAL REQUIREMENTS:<span class="ltx_text ltx_font_medium"></span
									></span>
								</p>
							</div>
							<div id="S4.SS2.p3" class="ltx_para">
								<ol id="S4.I3" class="ltx_enumerate">
									<li id="S4.I3.i1" class="ltx_item" style="list-style-type: none">
										<span class="ltx_tag ltx_tag_item">1.</span>
										<div id="S4.I3.i1.p1" class="ltx_para">
											<p class="ltx_p">
												<span class="ltx_text" style="font-size: 70%">Use </span
												><span class="ltx_text ltx_font_bold" style="font-size: 70%">ONLY</span
												><span class="ltx_text" style="font-size: 70%">
													the exact prefilled code snippet as starting point.</span
												>
											</p>
										</div>
									</li>
									<li id="S4.I3.i2" class="ltx_item" style="list-style-type: none">
										<span class="ltx_tag ltx_tag_item">2.</span>
										<div id="S4.I3.i2.p1" class="ltx_para">
											<p class="ltx_p">
												<span class="ltx_text ltx_font_bold" style="font-size: 70%">NO</span
												><span class="ltx_text" style="font-size: 70%">
													additional imports beyond what’s given.</span
												>
											</p>
										</div>
									</li>
									<li id="S4.I3.i3" class="ltx_item" style="list-style-type: none">
										<span class="ltx_tag ltx_tag_item">3.</span>
										<div id="S4.I3.i3.p1" class="ltx_para">
											<p class="ltx_p">
												<span class="ltx_text ltx_font_bold" style="font-size: 70%">NO</span
												><span class="ltx_text" style="font-size: 70%">
													sample/test data creation—use </span
												><span class="ltx_text ltx_font_bold" style="font-size: 70%">ONLY</span
												><span class="ltx_text" style="font-size: 70%">
													the provided DataFrame.</span
												>
											</p>
										</div>
									</li>
									<li id="S4.I3.i4" class="ltx_item" style="list-style-type: none">
										<span class="ltx_tag ltx_tag_item">4.</span>
										<div id="S4.I3.i4.p1" class="ltx_para">
											<p class="ltx_p">
												<span class="ltx_text ltx_font_bold" style="font-size: 70%">NO</span
												><span class="ltx_text" style="font-size: 70%">
													functions unless explicitly required in the original code.</span
												>
											</p>
										</div>
									</li>
									<li id="S4.I3.i5" class="ltx_item" style="list-style-type: none">
										<span class="ltx_tag ltx_tag_item">5.</span>
										<div id="S4.I3.i5.p1" class="ltx_para">
											<p class="ltx_p">
												<span class="ltx_text" style="font-size: 70%"
													>Code must end with appropriate visualization command (</span
												><span class="ltx_text ltx_font_typewriter" style="font-size: 70%"
													>plt.show()</span
												><span class="ltx_text" style="font-size: 70%"> for Matplotlib, </span
												><span class="ltx_text ltx_font_typewriter" style="font-size: 70%"
													>fig.show()</span
												><span class="ltx_text" style="font-size: 70%"> for Plotly, etc.).</span>
											</p>
										</div>
									</li>
									<li id="S4.I3.i6" class="ltx_item" style="list-style-type: none">
										<span class="ltx_tag ltx_tag_item">6.</span>
										<div id="S4.I3.i6.p1" class="ltx_para">
											<p class="ltx_p">
												<span class="ltx_text" style="font-size: 70%"
													>Code must be fully runnable without any modifications.</span
												>
											</p>
										</div>
									</li>
								</ol>
							</div>
							<div id="S4.SS2.p4" class="ltx_para">
								<p class="ltx_p">
									<span class="ltx_text ltx_font_bold" style="font-size: 70%"
										>PROBLEM:<span class="ltx_text ltx_font_medium"></span
									></span>
								</p>
							</div>
							<div id="S4.SS2.p5" class="ltx_para">
								<p class="ltx_p">
									<span class="ltx_text" style="font-size: 70%"
										>Title:
										<span class="ltx_text ltx_font_bold"
											>{<span class="ltx_text ltx_font_typewriter">Title</span>}</span
										></span
									>
								</p>
							</div>
							<div id="S4.SS2.p6" class="ltx_para">
								<p class="ltx_p">
									<span class="ltx_text" style="font-size: 70%"
										>Description:
										<span class="ltx_text ltx_font_bold"
											>{<span class="ltx_text ltx_font_typewriter">Description</span>}</span
										></span
									>
								</p>
							</div>
							<div id="S4.SS2.p7" class="ltx_para">
								<p class="ltx_p">
									<span class="ltx_text" style="font-size: 70%"
										>Difficulty:
										<span class="ltx_text ltx_font_bold"
											>{<span class="ltx_text ltx_font_typewriter">Level</span>}</span
										></span
									>
								</p>
							</div>
							<div id="S4.SS2.p8" class="ltx_para">
								<p class="ltx_p">
									<span class="ltx_text ltx_font_bold" style="font-size: 70%"
										>{<span class="ltx_text ltx_font_typewriter">DataFrame</span>}<span
											class="ltx_text ltx_font_medium"
										></span
									></span>
								</p>
							</div>
							<div id="S4.SS2.p9" class="ltx_para">
								<p class="ltx_p">
									<span class="ltx_text ltx_font_bold" style="font-size: 70%"
										>{<span class="ltx_text ltx_font_typewriter">Additional</span>}<span
											class="ltx_text ltx_font_medium"
										></span
									></span>
								</p>
							</div>
							<div id="S4.SS2.p10" class="ltx_para">
								<p class="ltx_p">
									<span class="ltx_text ltx_font_bold" style="font-size: 70%"
										>EXPECTED OUTPUT FORMAT:<span class="ltx_text ltx_font_medium"></span
									></span>
								</p>
							</div>
							<div id="S4.SS2.p11" class="ltx_para">
								<ul id="S4.I4" class="ltx_itemize">
									<li id="S4.I4.i1" class="ltx_item" style="list-style-type: none">
										<span class="ltx_tag ltx_tag_item">•</span>
										<div id="S4.I4.i1.p1" class="ltx_para">
											<p class="ltx_p">
												<span class="ltx_text" style="font-size: 70%"
													>Direct plotting code only.</span
												>
											</p>
										</div>
									</li>
									<li id="S4.I4.i2" class="ltx_item" style="list-style-type: none">
										<span class="ltx_tag ltx_tag_item">•</span>
										<div id="S4.I4.i2.p1" class="ltx_para">
											<p class="ltx_p">
												<span class="ltx_text" style="font-size: 70%"
													>Must end with appropriate </span
												><span class="ltx_text ltx_font_typewriter" style="font-size: 70%"
													>show()</span
												><span class="ltx_text" style="font-size: 70%"> command.</span>
											</p>
										</div>
									</li>
									<li id="S4.I4.i3" class="ltx_item" style="list-style-type: none">
										<span class="ltx_tag ltx_tag_item">•</span>
										<div id="S4.I4.i3.p1" class="ltx_para">
											<p class="ltx_p">
												<span class="ltx_text" style="font-size: 70%">No </span
												><span class="ltx_text ltx_font_typewriter" style="font-size: 70%"
													>return</span
												><span class="ltx_text" style="font-size: 70%"> statements.</span>
											</p>
										</div>
									</li>
									<li id="S4.I4.i4" class="ltx_item" style="list-style-type: none">
										<span class="ltx_tag ltx_tag_item">•</span>
										<div id="S4.I4.i4.p1" class="ltx_para">
											<p class="ltx_p">
												<span class="ltx_text" style="font-size: 70%">No </span
												><span class="ltx_text ltx_font_typewriter" style="font-size: 70%"
													>print</span
												><span class="ltx_text" style="font-size: 70%"> statements.</span>
											</p>
										</div>
									</li>
									<li id="S4.I4.i5" class="ltx_item" style="list-style-type: none">
										<span class="ltx_tag ltx_tag_item">•</span>
										<div id="S4.I4.i5.p1" class="ltx_para">
											<p class="ltx_p">
												<span class="ltx_text" style="font-size: 70%"
													>No functions unless explicitly required.</span
												>
											</p>
										</div>
									</li>
									<li id="S4.I4.i6" class="ltx_item" style="list-style-type: none">
										<span class="ltx_tag ltx_tag_item">•</span>
										<div id="S4.I4.i6.p1" class="ltx_para">
											<p class="ltx_p">
												<span class="ltx_text" style="font-size: 70%">No test data creation.</span>
											</p>
										</div>
									</li>
								</ul>
							</div>
							<div id="S4.SS2.p12" class="ltx_para">
								<p class="ltx_p">
									<span class="ltx_text ltx_font_bold" style="font-size: 70%"
										>Prefilled code snippet (use exactly):<span
											class="ltx_text ltx_font_medium"
										></span
									></span>
								</p>
							</div>
							<div id="S4.SS2.p13" class="ltx_para">
								<p class="ltx_p">
									<span class="ltx_text ltx_font_bold" style="font-size: 70%"
										>{<span class="ltx_text ltx_font_typewriter">Code</span>}<span
											class="ltx_text ltx_font_medium"
										>
										</span
									></span>
								</p>
							</div>
							<div id="S4.SS2.p14" class="ltx_para">
								<p class="ltx_p">
									This step started by selecting one problem from each task category-Analytical,
									Algorithm, and Visualization-for prompt development. These problems were outside
									our main dataset to avoid biasing the evaluation results. During this phase, we
									experimented with various prompt structures and observed the models’ outputs.
									Initially, the models often generated code that included datasets or functions not
									specified in the problem descriptions. To address this, we iteratively refined the
									prompts by introducing specific instructions and constraints.
								</p>
							</div>
							<div id="S4.SS2.p15" class="ltx_para">
								<p class="ltx_p">
									We tested several LLMs during prompt engineering, including some not selected for
									the main experiment. Some LLMs, such as Gemini (1.5 Flash), could not produce
									functional code even for easy problems, despite multiple prompt refinements.
									Others, like YouChat Pro, was capable but was not included in the final selection
									to avoid redundancy.
								</p>
							</div>
							<div id="S4.SS2.p16" class="ltx_para">
								<p class="ltx_p">
									To ensure consistency and minimize subjectivity, we automated the conversion of
									problem descriptions into prompts. This involved creating prompt templates
									tailored to each problem type-Analytical, Algorithm, and Visualization-which
									addressed the unique requirements of each category. This section illustrates the
									prompt template used for Visualization problems. The templates for Algorithm and
									Analytics tasks are available in
									<cite class="ltx_cite ltx_citemacro_cite"
										>[<a
											href="#bib.bib76"
											title="LLM4DS-Benchmark: A Dataset for Assessing LLM Performance in Data Science Coding Tasks"
											class="ltx_ref"
											>2</a
										>]</cite
									>. Our automated prompt generation system parsed the problem descriptions and
									inserted the information into the appropriate template based on the problem type.
								</p>
							</div>
						</section>
						<section id="S4.SS3" class="ltx_subsection">
							<h3 class="ltx_title ltx_title_subsection">
								<span class="ltx_tag ltx_tag_subsection">IV-C </span
								><span class="ltx_text ltx_font_italic">Code Generation and Execution</span>
							</h3>

							<div id="S4.SS3.p1" class="ltx_para">
								<p class="ltx_p">
									In this experiment, we presented 100 problem prompts to four AI
									assistants—Microsoft Copilot, ChatGPT, Claude, and Perplexity Labs—generating a
									total of 400 code solutions (100 problems per assistant). For each problem, a new
									chat thread was initiated with the AI assistant to ensure no influence from
									previous interactions. Each AI assistant was given up to three attempts per
									problem, guided by feedback such as “Not worked” (which yielded better results
									with ChatGPT and Copilot) or “Wrong answer” (more effective with Claude and
									Perplexity) to prompt improvements.
								</p>
							</div>
							<div id="S4.SS3.p2" class="ltx_para">
								<p class="ltx_p">
									To evaluate the solutions, we executed them on the Stratascratch platform and
									recorded the metrics provided by the platform, depending on the type of problem.
									For visualization problems, for example, the platforms calculates the similarity
									of the generated graphs with the expected outputs. Figure <a
										href="#S4.F3"
										title="Figure 3 ‣ IV-C Code Generation and Execution ‣ IV Experiment Operation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">3</span></a
									>
									illustrates a similarity comparison between a graph generated by the Perplexity
									model and the expected solution provided by Stratascratch.
								</p>
							</div>
							<figure id="S4.F3" class="ltx_figure">
								<p class="ltx_p ltx_align_center">
									<span
										class="ltx_text ltx_framed ltx_framed_rectangle"
										style="border-color: #000000"
										><img
											src="x3.png"
											id="S4.F3.g1"
											class="ltx_graphics ltx_img_portrait"
											width="538"
											height="719"
											alt="Refer to caption"
									/></span>
								</p>
								<figcaption class="ltx_caption ltx_centering">
									<span class="ltx_tag ltx_tag_figure">Figure 3: </span>Similarity comparison for a
									Visualization Problem.
								</figcaption>
							</figure>
							<div id="S4.SS3.p3" class="ltx_para">
								<p class="ltx_p">
									Due to Stratascratch platform constraints (e.g., limitations on library imports
									and required code formatting), we allowed minor manual edits to adapt the
									AI-generated code for consistent evaluation. These adjustments included removing
									prohibited imports (e.g., import os), modifying code structure (e.g., removing
									function encapsulation when global code was needed), and eliminating unnecessary
									print statements in favor of returns. These edits preserved the core logic and
									functionality of the solutions and were documented for transparency and
									reproducibility. This documentation (available in
									<cite class="ltx_cite ltx_citemacro_cite"
										>[<a
											href="#bib.bib76"
											title="LLM4DS-Benchmark: A Dataset for Assessing LLM Performance in Data Science Coding Tasks"
											class="ltx_ref"
											>2</a
										>]</cite
									>) includes the nature of the edits and their reason.
								</p>
							</div>
						</section>
					</section>
					<section id="S5" class="ltx_section">
						<h2 class="ltx_title ltx_title_section">
							<span class="ltx_tag ltx_tag_section">V </span
							><span class="ltx_text ltx_font_smallcaps">Analysis and Interpretation</span>
						</h2>

						<div id="S5.p1" class="ltx_para">
							<p class="ltx_p">
								This section presents the statistical analysis of data collected during the
								experiment. The dataset includes information such as problem IDs, the code generated
								by each LLM, and associated performance metrics, which is available in full for
								reproducibility in
								<cite class="ltx_cite ltx_citemacro_cite"
									>[<a
										href="#bib.bib76"
										title="LLM4DS-Benchmark: A Dataset for Assessing LLM Performance in Data Science Coding Tasks"
										class="ltx_ref"
										>2</a
									>]</cite
								>.
							</p>
						</div>
						<div id="S5.p2" class="ltx_para">
							<p class="ltx_p">
								Figure
								<a
									href="#S5.F4"
									title="Figure 4 ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
									class="ltx_ref"
									><span class="ltx_text ltx_ref_tag">4</span></a
								>
								provides a general overview of the LLMs’ assertiveness across all tasks and
								difficulty levels. This initial visualization offers a preliminary look at overall
								trends, while more detailed analyses follow for each research question (RQ).
							</p>
						</div>
						<figure id="S5.F4" class="ltx_figure">
							<img
								src="x4.png"
								id="S5.F4.g1"
								class="ltx_graphics ltx_centering ltx_img_landscape"
								width="789"
								height="621"
								alt="Refer to caption"
							/>
							<figcaption class="ltx_caption ltx_centering">
								<span class="ltx_tag ltx_tag_figure">Figure 4: </span>Overall Success Rate of LLMs.
							</figcaption>
						</figure>
						<div id="S5.p3" class="ltx_para">
							<p class="ltx_p">
								For each research question (RQ), we begin by visualizing the data to provide an
								intuitive understanding of the performance distributions across different
								conditions. In addition to visualization and descriptive statistics, we perform
								hypothesis testing for each RQ.
							</p>
						</div>
						<section id="S5.SS1" class="ltx_subsection">
							<h3 class="ltx_title ltx_title_subsection">
								<span class="ltx_tag ltx_tag_subsection">V-A </span
								><span class="ltx_text ltx_font_italic"
									>RQ1: Success Rate of LLMs in Solving Data Science Problems</span
								>
							</h3>

							<figure id="S5.F5" class="ltx_figure">
								<img
									src="x5.png"
									id="S5.F5.g1"
									class="ltx_graphics ltx_centering ltx_img_landscape"
									width="747"
									height="445"
									alt="Refer to caption"
								/>
								<figcaption class="ltx_caption ltx_centering">
									<span class="ltx_tag ltx_tag_figure">Figure 5: </span>RQ1 - LLM success rate in
									solving DS coding problems.
								</figcaption>
							</figure>
							<div id="S5.SS1.p1" class="ltx_para">
								<p class="ltx_p">
									As shown in Figure
									<a
										href="#S5.F5"
										title="Figure 5 ‣ V-A RQ1: Success Rate of LLMs in Solving Data Science Problems ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">5</span></a
									>, ChatGPT achieves the highest success rate (72%), followed by Claude (70%) and
									Perplexity (66%), with Copilot at 60%. These percentages represent the proportion
									of correct solutions generated by each LLM, including those needing minor code
									edits.
								</p>
							</div>
							<div id="S5.SS1.p2" class="ltx_para">
								<p class="ltx_p">
									<span class="ltx_text ltx_font_bold">Hypothesis Testing</span>: To assess each
									model’s success rate, we conducted a one-tailed binomial test with baseline
									thresholds of 50%, 60%, and 70%, determining if each LLM’s success rate
									significantly exceeded these benchmarks. This non-parametric test, suitable for
									binary outcomes (correct/incorrect), provides insight into each LLM’s performance
									relative to random chance
									<cite class="ltx_cite ltx_citemacro_cite"
										>[<a
											href="#bib.bib24"
											title="Experimentation in software engineering"
											class="ltx_ref"
											>21</a
										>]</cite
									>. Additionally, we evaluated whether there was a significant difference in
									success rates between the LLMs by applying the Friedman test, followed by pairwise
									Wilcoxon tests where a significant difference was detected.
								</p>
							</div>
							<figure id="S5.T2" class="ltx_table">
								<figcaption class="ltx_caption ltx_centering">
									<span class="ltx_tag ltx_tag_table">TABLE II: </span>RQ1: Success rate results of
									LLMs at different baselines
								</figcaption>
								<table class="ltx_tabular ltx_centering ltx_align_middle">
									<tbody class="ltx_tbody">
										<tr class="ltx_tr">
											<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
												<span class="ltx_text ltx_font_bold" style="font-size: 70%">Baseline</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text ltx_font_bold" style="font-size: 70%">LLM</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text ltx_font_bold" style="font-size: 70%"
													>Success Rate (%)</span
												>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text ltx_font_bold" style="font-size: 70%">p-value</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text ltx_font_bold" style="font-size: 70%"
													>Conclusion</span
												>
											</td>
										</tr>
										<tr class="ltx_tr">
											<td
												class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"
												rowspan="4"
											>
												<span class="ltx_text" style="font-size: 70%">50%</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Copilot</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">60%</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">0.0284</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Significant</span>
											</td>
										</tr>
										<tr class="ltx_tr">
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">ChatGPT</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">72%</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">0.0000</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Significant</span>
											</td>
										</tr>
										<tr class="ltx_tr">
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Perplexity</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">66%</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">0.0009</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Significant</span>
											</td>
										</tr>
										<tr class="ltx_tr">
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Claude</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">70%</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">0.0000</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Significant</span>
											</td>
										</tr>
										<tr class="ltx_tr">
											<td
												class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"
												rowspan="4"
											>
												<span class="ltx_text" style="font-size: 70%">60%</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Copilot</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">60%</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">0.5433</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Not Significant</span>
											</td>
										</tr>
										<tr class="ltx_tr">
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">ChatGPT</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">72%</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">0.0084</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Significant</span>
											</td>
										</tr>
										<tr class="ltx_tr">
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Perplexity</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">66%</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">0.1303</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Not Significant</span>
											</td>
										</tr>
										<tr class="ltx_tr">
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Claude</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">70%</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">0.0248</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Significant</span>
											</td>
										</tr>
										<tr class="ltx_tr">
											<td
												class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"
												rowspan="4"
											>
												<span class="ltx_text" style="font-size: 70%">70%</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Copilot</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">60%</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">0.9875</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Not Significant</span>
											</td>
										</tr>
										<tr class="ltx_tr">
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">ChatGPT</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">72%</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">0.3768</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Not Significant</span>
											</td>
										</tr>
										<tr class="ltx_tr">
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Perplexity</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">66%</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">0.8371</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Not Significant</span>
											</td>
										</tr>
										<tr class="ltx_tr">
											<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Claude</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">70%</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">0.5491</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Not Significant</span>
											</td>
										</tr>
									</tbody>
								</table>
							</figure>
							<div id="S5.SS1.p3" class="ltx_para">
								<p class="ltx_p">
									As Table
									<a
										href="#S5.T2"
										title="TABLE II ‣ V-A RQ1: Success Rate of LLMs in Solving Data Science Problems ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">II</span></a
									>
									shows, all LLMs perform significantly above the 50% threshold, confirming baseline
									effectiveness in solving coding tasks. At the 60% baseline, only ChatGPT and
									Claude reach statistical significance, suggesting enhanced reliability for typical
									tasks. No LLM achieves significance at the 70% baseline, indicating limitations in
									sustaining very high success rates across diverse challenges.
								</p>
							</div>
							<figure id="S5.F6" class="ltx_figure">
								<img
									src="x6.png"
									id="S5.F6.g1"
									class="ltx_graphics ltx_centering ltx_img_square"
									width="623"
									height="509"
									alt="Refer to caption"
								/>
								<figcaption class="ltx_caption ltx_centering">
									<span class="ltx_tag ltx_tag_figure">Figure 6: </span>RQ1 - Pairwise Comparison of
									Success Rates.
								</figcaption>
							</figure>
							<div id="S5.SS1.p4" class="ltx_para">
								<p class="ltx_p">
									To explore differences between LLMs, we applied the Friedman test, which detected
									significant variation in success rates across models (p = 0.0384). We followed up
									with post-hoc Wilcoxon pairwise comparisons, identifying a statistically
									significant difference between ChatGPT and Copilot, with ChatGPT achieving a
									significantly higher success rate (corrected p-value: 0.0437), as depicted in the
									heatmap of Figure
									<a
										href="#S5.F6"
										title="Figure 6 ‣ V-A RQ1: Success Rate of LLMs in Solving Data Science Problems ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">6</span></a
									>. No other significant differences were observed among models.
								</p>
							</div>
							<div id="S5.SS1.p5" class="ltx_para">
								<p class="ltx_p">Based on these tests, we conclude:</p>
								<blockquote class="ltx_quote">
									<p class="ltx_p">
										<span class="ltx_text ltx_font_italic"
											>For hypotheses
											<math id="S5.SS1.p5.m1" class="ltx_Math" alttext="H0_{1}" display="inline"
												><mrow
													><mi>H</mi><mo>⁢</mo><msub><mn>0</mn><mn>1</mn></msub></mrow
												></math
											>
											and
											<math id="S5.SS1.p5.m2" class="ltx_Math" alttext="H0_{1a}" display="inline"
												><mrow
													><mi>H</mi><mo>⁢</mo
													><msub
														><mn>0</mn><mrow><mn>1</mn><mo>⁢</mo><mi>a</mi></mrow></msub
													></mrow
												></math
											>:</span
										>
									</p>
									<ul id="S5.I1" class="ltx_itemize">
										<li id="S5.I1.i1" class="ltx_item" style="list-style-type: none">
											<span class="ltx_tag ltx_tag_item">•</span>
											<div id="S5.I1.i1.p1" class="ltx_para">
												<p class="ltx_p">
													<span class="ltx_text ltx_font_italic">At the </span
													><span class="ltx_text ltx_font_bold ltx_font_italic">50% baseline</span
													><span class="ltx_text ltx_font_italic"
														>, all LLMs exhibit success rates significantly above 50%, supporting
														the conclusion that each model performs better than random chance in
														solving data science coding problems.</span
													>
												</p>
											</div>
										</li>
										<li id="S5.I1.i2" class="ltx_item" style="list-style-type: none">
											<span class="ltx_tag ltx_tag_item">•</span>
											<div id="S5.I1.i2.p1" class="ltx_para">
												<p class="ltx_p">
													<span class="ltx_text ltx_font_italic">At the </span
													><span class="ltx_text ltx_font_bold ltx_font_italic">60% baseline</span
													><span class="ltx_text ltx_font_italic"
														>, only ChatGPT and Claude show success rates significantly above this
														level, indicating that these two models exhibit greater reliability
														across general coding tasks.</span
													>
												</p>
											</div>
										</li>
										<li id="S5.I1.i3" class="ltx_item" style="list-style-type: none">
											<span class="ltx_tag ltx_tag_item">•</span>
											<div id="S5.I1.i3.p1" class="ltx_para">
												<p class="ltx_p">
													<span class="ltx_text ltx_font_italic">At the </span
													><span class="ltx_text ltx_font_bold ltx_font_italic">70% baseline</span
													><span class="ltx_text ltx_font_italic"
														>, no LLM meets statistical significance, suggesting a possible
														limitation in achieving consistently high success rates across diverse
														coding challenges.</span
													>
												</p>
											</div>
										</li>
										<li id="S5.I1.i4" class="ltx_item" style="list-style-type: none">
											<span class="ltx_tag ltx_tag_item">•</span>
											<div id="S5.I1.i4.p1" class="ltx_para">
												<p class="ltx_p">
													<span class="ltx_text ltx_font_bold ltx_font_italic"
														>Friedman Test and Wilcoxon Post-hoc Test:</span
													><span class="ltx_text ltx_font_italic">
														Significant differences were found between models, with ChatGPT
														achieving a success rate significantly higher than that of
														Copilot.</span
													>
												</p>
											</div>
										</li>
									</ul>
								</blockquote>
							</div>
							<div id="S5.SS1.p6" class="ltx_para">
								<p class="ltx_p">
									In summary, RQ1 indicates that ChatGPT and Claude exhibit the most consistent
									performance, particularly ChatGPT, which leads in relative success. These findings
									suggest that ChatGPT and Claude may be preferable for tasks demanding higher
									success rates, while highlighting the difficulty for LLMs in consistently
									achieving a 70% success rate across diverse challenges.
								</p>
							</div>
						</section>
						<section id="S5.SS2" class="ltx_subsection">
							<h3 class="ltx_title ltx_title_subsection">
								<span class="ltx_tag ltx_tag_subsection">V-B </span
								><span class="ltx_text ltx_font_italic"
									>RQ2: Does the difficulty level of coding problems (easy, medium, hard) influence
									the success rate of the different LLMs?</span
								>
							</h3>

							<figure id="S5.F7" class="ltx_figure">
								<img
									src="x7.png"
									id="S5.F7.g1"
									class="ltx_graphics ltx_centering ltx_img_landscape"
									width="789"
									height="469"
									alt="Refer to caption"
								/>
								<figcaption class="ltx_caption ltx_centering">
									<span class="ltx_tag ltx_tag_figure">Figure 7: </span>RQ2: Effect of difficulty
									level on success rate.
								</figcaption>
							</figure>
							<div id="S5.SS2.p1" class="ltx_para">
								<p class="ltx_p">
									As shown in Figure
									<a
										href="#S5.F7"
										title="Figure 7 ‣ V-B RQ2: Does the difficulty level of coding problems (easy, medium, hard) influence the success rate of the different LLMs? ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">7</span></a
									>, the success rates of each LLM vary across different difficulty levels. Claude
									achieves the highest success rate on easy and medium problems, while ChatGPT
									excels on hard problems, suggesting its robustness with advanced challenges.
									Copilot consistently shows the lowest success rate across all difficulty levels,
									indicating a potential limitation in handling more complex tasks.
								</p>
							</div>
							<div id="S5.SS2.p2" class="ltx_para">
								<p class="ltx_p">
									<span class="ltx_text ltx_font_bold">Hypothesis Testing</span>: Chi-Square tests
									were performed to evaluate the effect of difficulty level on each LLM’s success
									rate. Results show that difficulty level significantly impacts the success rates
									of Perplexity and Claude (<math
										id="S5.SS2.p2.m1"
										class="ltx_Math"
										alttext="p&lt;0.05"
										display="inline"
										><mrow><mi>p</mi><mo>&lt;</mo><mn>0.05</mn></mrow></math
									>), suggesting that their performance fluctuates with problem complexity. In
									contrast, Copilot and ChatGPT demonstrate consistent success rates across all
									difficulty levels, indicated by non-significant results.
								</p>
							</div>
							<div id="S5.SS2.p3" class="ltx_para">
								<p class="ltx_p">Based on these tests, we conclude:</p>
								<blockquote class="ltx_quote">
									<p class="ltx_p">
										<span class="ltx_text ltx_font_italic"
											>For the hypothesis H0_2, we reject it for Perplexity and Claude, indicating
											that difficulty level significantly affects their success rates. For Copilot
											and ChatGPT, we fail to reject H0_2, suggesting consistent performance across
											varying difficulty levels.</span
										>
									</p>
								</blockquote>
							</div>
							<div id="S5.SS2.p4" class="ltx_para">
								<p class="ltx_p">
									To further explore comparative performance, we conducted the Friedman test across
									all models at each difficulty level. Although the overall test did not show
									significant differences in success rates across LLMs for each level, pairwise
									Wilcoxon tests highlighted a significant difference between ChatGPT and Copilot
									for hard problems (p = 0.0196), indicating ChatGPT’s superior performance on more
									challenging tasks.
								</p>
							</div>
						</section>
						<section id="S5.SS3" class="ltx_subsection">
							<h3 class="ltx_title ltx_title_subsection">
								<span class="ltx_tag ltx_tag_subsection">V-C </span
								><span class="ltx_text ltx_font_italic"
									>RQ3: Does the type of data science task (Analytical, Algorithm, Visualization)
									influence the success rate of the different LLMs?</span
								>
							</h3>

							<figure id="S5.F8" class="ltx_figure">
								<img
									src="x8.png"
									id="S5.F8.g1"
									class="ltx_graphics ltx_centering ltx_img_landscape"
									width="789"
									height="469"
									alt="Refer to caption"
								/>
								<figcaption class="ltx_caption ltx_centering">
									<span class="ltx_tag ltx_tag_figure">Figure 8: </span>RQ3: Effect of task type on
									success rate.
								</figcaption>
							</figure>
							<div id="S5.SS3.p1" class="ltx_para">
								<p class="ltx_p">
									Figure
									<a
										href="#S5.F8"
										title="Figure 8 ‣ V-C RQ3: Does the type of data science task (Analytical, Algorithm, Visualization) influence the success rate of the different LLMs? ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">8</span></a
									>
									illustrates the success rate of each LLM across different task types. ChatGPT
									demonstrates the highest success rate in analytical and algorithm tasks, while
									Perplexity and Claude achieve similar levels in visualization tasks. Although
									ChatGPT performs particularly well in analytical and algorithm tasks, statistical
									tests reveal no significant overall success rate differences among the models
									across task types, except between ChatGPT and Copilot.
								</p>
							</div>
							<figure id="S5.T3" class="ltx_table">
								<figcaption class="ltx_caption ltx_centering">
									<span class="ltx_tag ltx_tag_table">TABLE III: </span>RQ3: Chi-Square test results
									for task type across LLMs
								</figcaption>
								<table class="ltx_tabular ltx_centering ltx_align_middle">
									<tbody class="ltx_tbody">
										<tr class="ltx_tr">
											<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
												<span class="ltx_text ltx_font_bold" style="font-size: 70%">LLM</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text ltx_font_bold" style="font-size: 70%">Algorithm</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text ltx_font_bold" style="font-size: 70%"
													>Analytical</span
												>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text ltx_font_bold" style="font-size: 70%">Visuali.</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text ltx_font_bold" style="font-size: 70%">p-value</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text ltx_font_bold" style="font-size: 70%"
													>Conclusion</span
												>
											</td>
										</tr>
										<tr class="ltx_tr">
											<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Copilot</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">22/35</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">17/35</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">21/30</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">0.1946</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Not Sig.</span>
											</td>
										</tr>
										<tr class="ltx_tr">
											<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">ChatGPT</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">26/35</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">25/35</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">21/30</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">0.9250</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Not Sig.</span>
											</td>
										</tr>
										<tr class="ltx_tr">
											<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Perplexity</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">23/35</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">20/35</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">23/30</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">0.2534</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Not Sig.</span>
											</td>
										</tr>
										<tr class="ltx_tr">
											<td
												class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"
											>
												<span class="ltx_text" style="font-size: 70%">Claude</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">26/35</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">21/35</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">23/30</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">0.2715</span>
											</td>
											<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
												<span class="ltx_text" style="font-size: 70%">Not Sig.</span>
											</td>
										</tr>
									</tbody>
								</table>
							</figure>
							<div id="S5.SS3.p2" class="ltx_para">
								<p class="ltx_p">
									The Chi-Square test results in Table
									<a
										href="#S5.T3"
										title="TABLE III ‣ V-C RQ3: Does the type of data science task (Analytical, Algorithm, Visualization) influence the success rate of the different LLMs? ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">III</span></a
									>
									show that task type does not significantly impact the success rate for any LLM,
									with all p-values exceeding the 0.05 threshold. This finding suggests that each
									model’s performance remains relatively stable across analytical, algorithmic, and
									visualization tasks.
								</p>
							</div>
							<div id="S5.SS3.p3" class="ltx_para">
								<blockquote class="ltx_quote">
									<p class="ltx_p">
										<span class="ltx_text ltx_font_italic"
											>For hypothesis H0_3, we fail to reject it for all models, indicating that
											task type does not significantly impact success rate overall. However,
											post-hoc comparisons reveal that ChatGPT performs significantly better than
											Copilot in analytical and algorithm tasks.</span
										>
									</p>
								</blockquote>
							</div>
						</section>
						<section id="S5.SS4" class="ltx_subsection">
							<h3 class="ltx_title ltx_title_subsection">
								<span class="ltx_tag ltx_tag_subsection">V-D </span
								><span class="ltx_text ltx_font_italic"
									>RQ4: For Analytical questions, do the LLMs differ in the efficiency (running
									time) of the code they generate?</span
								>
							</h3>

							<figure id="S5.F9" class="ltx_figure">
								<img
									src="x9.png"
									id="S5.F9.g1"
									class="ltx_graphics ltx_centering ltx_img_landscape"
									width="831"
									height="480"
									alt="Refer to caption"
								/>
								<figcaption class="ltx_caption ltx_centering">
									<span class="ltx_tag ltx_tag_figure">Figure 9: </span>RQ4: Execution times of LLMs
									- Box Plot.
								</figcaption>
							</figure>
							<figure id="S5.F10" class="ltx_figure">
								<img
									src="x10.png"
									id="S5.F10.g1"
									class="ltx_graphics ltx_centering ltx_img_landscape"
									width="748"
									height="479"
									alt="Refer to caption"
								/>
								<figcaption class="ltx_caption ltx_centering">
									<span class="ltx_tag ltx_tag_figure">Figure 10: </span>RQ4: Median execution time
									by difficulty level.
								</figcaption>
							</figure>
							<div id="S5.SS4.p1" class="ltx_para">
								<p class="ltx_p">
									For a fair comparison, Figures
									<a
										href="#S5.F9"
										title="Figure 9 ‣ V-D RQ4: For Analytical questions, do the LLMs differ in the efficiency (running time) of the code they generate? ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">9</span></a
									>
									and
									<a
										href="#S5.F10"
										title="Figure 10 ‣ V-D RQ4: For Analytical questions, do the LLMs differ in the efficiency (running time) of the code they generate? ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">10</span></a
									>
									include only the results from problems successfully solved by all LLMs, as the
									platform does not compute execution times for solutions that did not work.
									Accordingly, Claude has the lowest median execution time, indicating its solutions
									generally execute faster than the other models’ solutions, followed by Copilot and
									Perplexity. ChatGPT has the highest median execution time, suggesting that on
									average, it takes longer to execute analytical tasks than the other models.
									ChatGPT displays the largest interquartile range (IQR), indicating significant
									variability, whereas Copilot, Perplexity, and Claude have narrower IQRs,
									suggesting more consistent execution times. This analysis suggests Claude is
									generally faster and more consistent for analytical tasks, while ChatGPT may offer
									less predictability in execution time.
								</p>
							</div>
							<div id="S5.SS4.p2" class="ltx_para">
								<p class="ltx_p">
									<span class="ltx_text ltx_font_bold">Hypothesis Testing</span>: To assess whether
									these observed differences are statistically significant, we conducted a
									Kruskal-Wallis test, as the Kruskal-Wallis test is a non-parametric method
									suitable for comparing the distributions of independent groups, particularly their
									central tendencies, when data is not normally distributed.
								</p>
							</div>
							<div id="S5.SS4.p3" class="ltx_para">
								<p class="ltx_p">
									The test, conducted using the
									<span class="ltx_text ltx_font_typewriter">scipy.stats</span> library, resulted in
									a Kruskal-Wallis statistic of 0.6947 and a p-value of 0.8744. With a p-value
									exceeding the significance level of 0.05, we fail to reject the null hypothesis.
									This suggests that there are no statistically significant differences in the
									median execution times across the LLMs for Analytical questions.
								</p>
							</div>
							<div id="S5.SS4.p4" class="ltx_para">
								<blockquote class="ltx_quote">
									<p class="ltx_p">
										<span class="ltx_text ltx_font_italic"
											>For RQ4, we fail to reject H0_4, indicating that the LLMs do not differ
											significantly in the efficiency (running time) of the code they generate for
											Analytical questions.</span
										>
									</p>
								</blockquote>
							</div>
						</section>
						<section id="S5.SS5" class="ltx_subsection">
							<h3 class="ltx_title ltx_title_subsection">
								<span class="ltx_tag ltx_tag_subsection">V-E </span
								><span class="ltx_text ltx_font_italic"
									>RQ5: For visualization tasks, do the LLMs differ in the quality (similarity) of
									the visual outputs they produce compared to expected results?</span
								>
							</h3>

							<figure id="S5.F11" class="ltx_figure">
								<img
									src="x11.png"
									id="S5.F11.g1"
									class="ltx_graphics ltx_centering ltx_img_landscape"
									width="831"
									height="487"
									alt="Refer to caption"
								/>
								<figcaption class="ltx_caption ltx_centering">
									<span class="ltx_tag ltx_tag_figure">Figure 11: </span>RQ5: Similarity Scores -
									Box Plot.
								</figcaption>
							</figure>
							<figure id="S5.F12" class="ltx_figure">
								<img
									src="x12.png"
									id="S5.F12.g1"
									class="ltx_graphics ltx_centering ltx_img_landscape"
									width="830"
									height="528"
									alt="Refer to caption"
								/>
								<figcaption class="ltx_caption ltx_centering">
									<span class="ltx_tag ltx_tag_figure">Figure 12: </span>RQ5: Median similarity
									scores by difficulty level.
								</figcaption>
							</figure>
							<div id="S5.SS5.p1" class="ltx_para">
								<p class="ltx_p">
									As depicted in Figures
									<a
										href="#S5.F11"
										title="Figure 11 ‣ V-E RQ5: For visualization tasks, do the LLMs differ in the quality (similarity) of the visual outputs they produce compared to expected results? ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">11</span></a
									>
									and
									<a
										href="#S5.F12"
										title="Figure 12 ‣ V-E RQ5: For visualization tasks, do the LLMs differ in the quality (similarity) of the visual outputs they produce compared to expected results? ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">12</span></a
									>, ChatGPT achieves the highest median similarity score among the commonly solved
									problems, indicating that its outputs are closest to the expected results.
									Additionally, ChatGPT displays the narrowest interquartile range (IQR),
									highlighting its consistency. These findings suggest that ChatGPT delivers more
									reliable quality in generating visual outputs that closely match the expected
									results.
								</p>
							</div>
							<div id="S5.SS5.p2" class="ltx_para">
								<p class="ltx_p">
									<span class="ltx_text ltx_font_bold">Hypothesis Testing</span>: To statistically
									analyze differences in similarity scores among the LLMs, we conducted a
									Kruskal-Wallis test.
								</p>
							</div>
							<div id="S5.SS5.p3" class="ltx_para">
								<p class="ltx_p">
									<span class="ltx_text ltx_font_bold">Kruskal-Wallis Test Results</span>:
								</p>
								<ul id="S5.I2" class="ltx_itemize">
									<li id="S5.I2.i1" class="ltx_item" style="list-style-type: none">
										<span class="ltx_tag ltx_tag_item">•</span>
										<div id="S5.I2.i1.p1" class="ltx_para">
											<p class="ltx_p">Kruskal-Wallis Statistic: 0.8287</p>
										</div>
									</li>
									<li id="S5.I2.i2" class="ltx_item" style="list-style-type: none">
										<span class="ltx_tag ltx_tag_item">•</span>
										<div id="S5.I2.i2.p1" class="ltx_para">
											<p class="ltx_p">p-value: 0.8426</p>
										</div>
									</li>
									<li id="S5.I2.i3" class="ltx_item" style="list-style-type: none">
										<span class="ltx_tag ltx_tag_item">•</span>
										<div id="S5.I2.i3.p1" class="ltx_para">
											<p class="ltx_p">
												<span class="ltx_text ltx_font_bold">Conclusion</span>: The p-value above
												0.05 suggests no statistically significant differences in similarity scores
												between the LLMs. This indicates that while there are observed differences
												in mean similarity scores and variability (with ChatGPT achieving the
												highest mean and most consistent performance), these differences are not
												statistically significant across LLMs at the 5% significance level.
											</p>
										</div>
									</li>
								</ul>
							</div>
							<div id="S5.SS5.p4" class="ltx_para">
								<p class="ltx_p">Based on these results, we conclude:</p>
								<blockquote class="ltx_quote">
									<p class="ltx_p">
										<span class="ltx_text ltx_font_italic"
											>For hypothesis H0_5, we fail to reject the null hypothesis, indicating that
											there is no significant difference in the similarity quality of generated
											visualization outputs among the LLMs.</span
										>
									</p>
								</blockquote>
							</div>
						</section>
						<section id="S5.SS6" class="ltx_subsection">
							<h3 class="ltx_title ltx_title_subsection">
								<span class="ltx_tag ltx_tag_subsection">V-F </span
								><span class="ltx_text ltx_font_italic">Threats to Validity</span>
							</h3>

							<div id="S5.SS6.p1" class="ltx_para">
								<p class="ltx_p">
									As usual in empirical studies, our study acknowledges several threats that may
									impact the interpretation and generalization of the results.
								</p>
							</div>
							<section id="S5.SS6.SSS1" class="ltx_subsubsection">
								<h4 class="ltx_title ltx_title_subsubsection">
									<span class="ltx_tag ltx_tag_subsubsection">V-F1 </span>Internal Validity
								</h4>

								<div id="S5.SS6.SSS1.p1" class="ltx_para">
									<p class="ltx_p">
										A key concern is the undisclosed nature of the LLMs’ training data. Without
										access to this information, we cannot confirm whether the generated solutions
										are novel or based on memorized content. Even though we selected new problems
										from Stratascratch, similar or identical problems might exist in the models’
										training data, potentially inflating their apparent effectiveness.
									</p>
								</div>
								<div id="S5.SS6.SSS1.p2" class="ltx_para">
									<p class="ltx_p">
										Prompt design is another factor influencing outcomes. As noted by White et al.
										<cite class="ltx_cite ltx_citemacro_cite"
											>[<a
												href="#bib.bib36"
												title="ChatGPT prompt patterns for improving code quality, refactoring, requirements elicitation, and software design"
												class="ltx_ref"
												>20</a
											>]</cite
										>, the formulation of prompts can significantly affect LLM outputs. While we
										endeavored to use consistent prompts derived from original problem descriptions,
										variations could lead to different results.
									</p>
								</div>
								<div id="S5.SS6.SSS1.p3" class="ltx_para">
									<p class="ltx_p">
										To address potential subjectivity in converting problems to prompts, we
										developed standardized prompt templates for each task type. These templates
										ensured that all AI assistants received clear, consistent instructions, allowing
										for a fair comparison of performance.
									</p>
								</div>
							</section>
							<section id="S5.SS6.SSS2" class="ltx_subsubsection">
								<h4 class="ltx_title ltx_title_subsubsection">
									<span class="ltx_tag ltx_tag_subsubsection">V-F2 </span>External Validity
								</h4>

								<div id="S5.SS6.SSS2.p1" class="ltx_para">
									<p class="ltx_p">
										The generalizability of our findings is limited by the scope of problems used.
										Our study focused on 100 Python coding problems from a single platform, which
										may not represent the full spectrum of data science tasks. To enhance external
										validity, future research should incorporate a wider range of problems from
										multiple sources.
									</p>
								</div>
							</section>
							<section id="S5.SS6.SSS3" class="ltx_subsubsection">
								<h4 class="ltx_title ltx_title_subsubsection">
									<span class="ltx_tag ltx_tag_subsubsection">V-F3 </span>Construct Validity
								</h4>

								<div id="S5.SS6.SSS3.p1" class="ltx_para">
									<p class="ltx_p">
										We did not formally assess the expertise of the researchers conducting the
										experiment, which could introduce subjectivity, particularly in interpreting and
										evaluating the AI-generated code. Although guidelines were established for
										acceptable code modifications—allowing only minor edits to resolve execution
										issues—differences in coding proficiency among researchers could influence the
										assessment.
									</p>
								</div>
							</section>
							<section id="S5.SS6.SSS4" class="ltx_subsubsection">
								<h4 class="ltx_title ltx_title_subsubsection">
									<span class="ltx_tag ltx_tag_subsubsection">V-F4 </span>Conclusion Validity
								</h4>

								<div id="S5.SS6.SSS4.p1" class="ltx_para">
									<p class="ltx_p">
										These threats may affect the validity of our conclusions. While our study offers
										insights into the capabilities and limitations of LLMs in data science code
										generation, the results should be interpreted with caution. Further research
										addressing these limitations is necessary to strengthen the confidence in the
										findings.
									</p>
								</div>
							</section>
						</section>
					</section>
					<section id="S6" class="ltx_section">
						<h2 class="ltx_title ltx_title_section">
							<span class="ltx_tag ltx_tag_section">VI </span
							><span class="ltx_text ltx_font_smallcaps">Discussion</span>
						</h2>

						<div id="S6.p1" class="ltx_para">
							<p class="ltx_p">
								Through a series of hypothesis tests, we investigated each model’s effectiveness
								across different problem types and difficulty levels. The findings underscore both
								the strengths and limitations of these LLMs in addressing data science challenges,
								providing insights into which models may be most suitable for specific scenarios in
								data science workflows. The results highlight that:
							</p>
							<ul id="S6.I1" class="ltx_itemize">
								<li id="S6.I1.i1" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">•</span>
									<div id="S6.I1.i1.p1" class="ltx_para">
										<p class="ltx_p">
											<span class="ltx_text ltx_font_bold">Success Rate:</span> Empirical evidence
											from our tests indicates that each LLM exceeded the 50% baseline success rate,
											confirming effectiveness beyond random chance. At the 60% baseline, only
											ChatGPT and Claude achieved significantly higher success rates, reinforcing
											their reliability in general coding contexts. However, none of the models
											reached the 70% threshold, suggesting limitations in consistently achieving
											high accuracy across diverse data science task types. ChatGPT achieved the
											highest overall success rate and performed consistently well on harder
											questions, with descriptive analysis suggesting strong outcomes in analytical
											and algorithmic tasks, reflecting its robustness in complex data science
											scenarios. Claude also demonstrated solid performance, particularly on easier
											and medium-difficulty tasks, as well as in visualization tasks, indicating
											versatility across various problem types. Perplexity and Copilot, while
											showing lower success rates on more complex tasks, displayed consistent
											performance on simpler tasks, highlighting their potential for straightforward
											data science workflows.
										</p>
									</div>
								</li>
								<li id="S6.I1.i2" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">•</span>
									<div id="S6.I1.i2.p1" class="ltx_para">
										<p class="ltx_p">
											<span class="ltx_text ltx_font_bold">Efficiency (Execution Time):</span> For
											analytical tasks, the Kruskal-Wallis test on execution times revealed no
											statistically significant differences among the models, suggesting that
											efficiency, in terms of runtime, is relatively comparable across these LLMs.
											This finding implies that while execution time may vary, it may not be a
											decisive factor in model selection for tasks where accuracy and complexity are
											primary concerns. Despite the lack of empirical significance, the median
											execution times indicate some practical trends: Claude had the lowest median
											execution time, suggesting it generally runs faster than the other models,
											followed by Copilot and Perplexity. ChatGPT had the highest median execution
											time, indicating slower performance on average.
										</p>
									</div>
								</li>
								<li id="S6.I1.i3" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">•</span>
									<div id="S6.I1.i3.p1" class="ltx_para">
										<p class="ltx_p">
											<span class="ltx_text ltx_font_bold"
												>Quality of Output (Image Similarity for Visualization):</span
											>
											In visualization tasks, where models were evaluated based on similarity scores
											to expected outputs, ChatGPT achieved the highest median similarity score.
											However, statistical tests indicated no significant differences between the
											models.
										</p>
									</div>
								</li>
								<li id="S6.I1.i4" class="ltx_item" style="list-style-type: none">
									<span class="ltx_tag ltx_tag_item">•</span>
									<div id="S6.I1.i4.p1" class="ltx_para">
										<p class="ltx_p">
											<span class="ltx_text ltx_font_bold"
												>Consistency Across Difficulty Levels and Task Types:</span
											>
											Empirical analysis reveals that ChatGPT maintains consistent performance
											regardless of task complexity, providing reliable success rates across both
											simple and complex tasks. In contrast, Perplexity’s and Claude’s success rates
											were significantly influenced by task difficulty, with better outcomes on less
											complex tasks. Copilot also demonstrated consistency across difficulty levels,
											though with generally lower success rates than ChatGPT. Additionally, our
											tests indicate that task type (analytical, algorithmic, visualization) does
											not significantly impact success rates for any model, suggesting stable
											performance across different data science task types. This consistency makes
											ChatGPT a dependable choice when task complexity is uncertain.
										</p>
									</div>
								</li>
							</ul>
						</div>
					</section>
					<section id="S7" class="ltx_section">
						<h2 class="ltx_title ltx_title_section">
							<span class="ltx_tag ltx_tag_section">VII </span
							><span class="ltx_text ltx_font_smallcaps">Conclusion</span>
						</h2>

						<div id="S7.p1" class="ltx_para">
							<p class="ltx_p">
								This study presents a controlled experiment evaluating the effectiveness of four
								prominent LLM-based AI assistants—Microsoft Copilot (GPT-4 Turbo), ChatGPT
								(o1-preview), Claude (3.5 Sonnet), and Perplexity Labs (Llama-3.1-70b-instruct)—in
								data science coding tasks. Effectiveness was measured by each model’s success rate,
								execution efficiency, visual output quality, and consistency across difficulty
								levels and task types.
							</p>
						</div>
						<div id="S7.p2" class="ltx_para">
							<p class="ltx_p">
								With success rates exceeding 50% for all models, this research provides valuable
								insights into LLM performance in data science. At the 60% baseline, only ChatGPT and
								Claude achieved significantly higher success rates, highlighting their reliability
								in general coding tasks. However, our findings indicate that only ChatGPT
								consistently maintains performance across different difficulty levels, whereas
								Claude’s success rate is significantly affected by task difficulty, suggesting its
								performance may vary with more complex tasks.
							</p>
						</div>
						<div id="S7.p3" class="ltx_para">
							<p class="ltx_p">
								No evidence suggests that task type affects LLM success rates, though ChatGPT
								(o1-preview) significantly outperforms Copilot (GPT-4o) for analytical and algorithm
								tasks. This nuanced understanding of each model’s strengths enables more strategic
								LLM selection tailored to specific needs. Additionally, this study underscores the
								value of rigorous hypothesis testing in AI evaluation, setting a template for
								assessing models beyond basic accuracy metrics.
							</p>
						</div>
					</section>
					<section id="S8" class="ltx_section">
						<h2 class="ltx_title ltx_title_section">
							<span class="ltx_tag ltx_tag_section">VIII </span
							><span class="ltx_text ltx_font_smallcaps">Future Work</span>
						</h2>

						<div id="S8.p1" class="ltx_para">
							<p class="ltx_p">
								Our study opens several avenues for future research to enhance the application of
								LLMs in data science code generation.
							</p>
						</div>
						<section id="S8.SS1" class="ltx_subsection">
							<h3 class="ltx_title ltx_title_subsection">
								<span class="ltx_tag ltx_tag_subsection">VIII-A </span
								><span class="ltx_text ltx_font_italic"
									>Exploring Complex and Real-World Data Science Tasks</span
								>
							</h3>

							<div id="S8.SS1.p1" class="ltx_para">
								<p class="ltx_p">
									Evaluating LLMs on sophisticated, real-world data science tasks—such as
									implementing machine learning models with libraries like Scikit-learn or
									TensorFlow, handling large datasets, and working with unstructured data—could
									provide deeper insights into their capabilities and limitations. For instance,
									Nascimento et al.
									<cite class="ltx_cite ltx_citemacro_cite"
										>[<a
											href="#bib.bib23"
											title="GPT-in-the-loop: supporting adaptation in multiagent systems"
											class="ltx_ref"
											>13</a
										>]</cite
									>
									demonstrated the use of an LLM to replace a learning algorithm that involved
									neural networks optimized through genetic algorithms. While their experiment was
									preliminary, it highlighted the potential of LLMs to automate complex coding
									solutions, suggesting that these models could extend beyond basic scripting to
									more advanced tasks. Exploring tasks like multivariate analysis, time series
									forecasting, and dynamic optimization could further test LLM proficiency. Testing
									in practical settings uncovers challenges that controlled experiments may not
									fully capture.
								</p>
							</div>
						</section>
						<section id="S8.SS2" class="ltx_subsection">
							<h3 class="ltx_title ltx_title_subsection">
								<span class="ltx_tag ltx_tag_subsection">VIII-B </span
								><span class="ltx_text ltx_font_italic"
									>Expanding Model Diversity and Dataset Coverage</span
								>
							</h3>

							<div id="S8.SS2.p1" class="ltx_para">
								<p class="ltx_p">
									We could extend this analysis by integrating additional LLMs and incorporating
									data science-specific coding challenges from various platforms, such as LeetCode,
									with tasks like data manipulation, cleaning, and SQL queries. To capture a broader
									range of data science skills, the dataset could also include non-coding tasks,
									such as interpretation and analysis questions, as provided by Stratascratch.
								</p>
							</div>
							<div id="S8.SS2.p2" class="ltx_para">
								<p class="ltx_p">
									Additionally, we could integrate recently released questions in Stratascratch that
									use Polars DataFrame
									<cite class="ltx_cite ltx_citemacro_cite"
										>[<a
											href="#bib.bib77"
											title="Polars: blazingly fast dataframes in rust, python, node.js, r, and sql"
											class="ltx_ref"
											>17</a
										>]</cite
									>
									for data manipulation-a high-performance library designed for efficient data
									handling in Python. We could further expand the dataset by incorporating problems
									from DS-1000
									<cite class="ltx_cite ltx_citemacro_cite"
										>[<a
											href="#bib.bib72"
											title="DS-1000: a natural and reliable benchmark for data science code generation"
											class="ltx_ref"
											>10</a
										>]</cite
									>, which includes a diverse selection of data science problems sourced from
									StackOverflow. Following recommendations by Lai et al.
									<cite class="ltx_cite ltx_citemacro_cite"
										>[<a
											href="#bib.bib72"
											title="DS-1000: a natural and reliable benchmark for data science code generation"
											class="ltx_ref"
											>10</a
										>]</cite
									>, introducing customized variations of existing problems would help reduce model
									memorization, enhancing the rigor of the evaluation environment.
								</p>
							</div>
						</section>
						<section id="S8.SS3" class="ltx_subsection">
							<h3 class="ltx_title ltx_title_subsection">
								<span class="ltx_tag ltx_tag_subsection">VIII-C </span
								><span class="ltx_text ltx_font_italic">Expanding Evaluation Metrics</span>
							</h3>

							<div id="S8.SS3.p1" class="ltx_para">
								<p class="ltx_p">
									Future work could expand LLM evaluation by integrating software engineering
									metrics like code complexity, maintainability, and readability. Code similarity
									analysis could assess alignment with industry standards, while qualitative reviews
									by data scientists would add valuable insights, particularly for visualization
									tasks where image similarity metrics may fall short.
								</p>
							</div>
						</section>
						<section id="S8.SS4" class="ltx_subsection">
							<h3 class="ltx_title ltx_title_subsection">
								<span class="ltx_tag ltx_tag_subsection">VIII-D </span
								><span class="ltx_text ltx_font_italic"
									>Investigating Prompt Engineering and Ensuring Reproducibility</span
								>
							</h3>

							<div id="S8.SS4.p1" class="ltx_para">
								<p class="ltx_p">
									Prompt engineering significantly influences LLM outputs. Future research should
									examine how different prompt formulations affect code generation quality and
									consistency. Employing methodologies where LLMs simulate multiple users
									<cite class="ltx_cite ltx_citemacro_cite"
										>[<a
											href="#bib.bib15"
											title="Using large language models to simulate multiple humans and replicate human subject studies"
											class="ltx_ref"
											>1</a
										>]</cite
									>
									could shed light on the impact of varying professional experiences and prompt
									designs. Addressing the non-deterministic nature of LLMs by controlling parameters
									like temperature settings could improve reproducibility, leading to more
									consistent and reliable evaluations.
								</p>
							</div>
						</section>
						<section id="S8.SS5" class="ltx_subsection">
							<h3 class="ltx_title ltx_title_subsection">
								<span class="ltx_tag ltx_tag_subsection">VIII-E </span
								><span class="ltx_text ltx_font_italic">Exploring Further Research Questions</span>
							</h3>

							<div id="S8.SS5.p1" class="ltx_para">
								<p class="ltx_p">
									Even using the same dataset, many additional questions and hypotheses remain to be
									explored. For instance, while we assessed the impact of problem difficulty and
									type on LLM success rates, further analysis could focus on establishing baseline
									success rates for each problem type and difficulty level. Given the general
									success baseline of 60%, future research might explore optimal baseline thresholds
									specific to each type and level of task. Beyond success rates, this dataset also
									allows for an in-depth exploration of efficiency (execution times) and similarity
									scores for each difficulty level, providing a more comprehensive view of model
									performance in diverse task complexity. Additionally, information on the number of
									attempts (up to three) and instances of minor edits provides data for assessing
									error types (syntax and logic errors), retry patterns, and the models’
									adaptability to user feedback. Our dataset also includes specific topics within
									each question type, allowing for a more granular analysis that could reveal
									topic-specific strengths or limitations of each LLM.
								</p>
							</div>
						</section>
					</section>
					<section id="bib" class="ltx_bibliography">
						<h2 class="ltx_title ltx_title_bibliography">References</h2>

						<ul id="bib.L1" class="ltx_biblist">
							<li id="bib.bib15" class="ltx_bibitem ltx_bib_inproceedings">
								<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[1]</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_author"
										>G. V. Aher, R. I. Arriaga, and A. T. Kalai</span
									><span class="ltx_text ltx_bib_year"> (2023)</span>
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_title"
										>Using large language models to simulate multiple humans and replicate human
										subject studies</span
									>.
								</span>
								<span class="ltx_bibblock"
									>In
									<span class="ltx_text ltx_bib_inbook"
										>International Conference on Machine Learning</span
									>,
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_pages"> pp. 337–371</span>.
								</span>
								<span class="ltx_bibblock ltx_bib_cited"
									>Cited by:
									<a
										href="#S8.SS4.p1"
										title="VIII-D Investigating Prompt Engineering and Ensuring Reproducibility ‣ VIII Future Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§VIII-D</span></a
									>.
								</span>
							</li>
							<li id="bib.bib76" class="ltx_bibitem ltx_bib_misc">
								<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[2]</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_author"
										>S. A. Boominathan, S. S. Chintakunta, N. Nascimento, and E. Guimaraes</span
									><span class="ltx_text ltx_bib_year"> (2024-11)</span>
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_title"
										>LLM4DS-Benchmark: A Dataset for Assessing LLM Performance in Data Science
										Coding Tasks</span
									>.
								</span>
								<span class="ltx_bibblock">
									<span class="ltx_text ltx_bib_publisher">Zenodo</span>.
								</span>
								<span class="ltx_bibblock"
									>External Links:
									<span class="ltx_text ltx_bib_links"
										><a
											href="https://dx.doi.org/10.5281/zenodo.14064111"
											title=""
											class="ltx_ref doi ltx_bib_external"
											>Document</a
										>,
										<a
											href="https://doi.org/10.5281/zenodo.14064111"
											title=""
											class="ltx_ref ltx_bib_external"
											>Link</a
										></span
									>
								</span>
								<span class="ltx_bibblock ltx_bib_cited"
									>Cited by:
									<a
										href="#S4.SS1.p7"
										title="IV-A Dataset: Selection of Data Science Problems ‣ IV Experiment Operation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§IV-A</span></a
									>,
									<a
										href="#S4.SS2.p16"
										title="IV-B Prompt Engineering: Transforming Problems into Prompts ‣ IV Experiment Operation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§IV-B</span></a
									>,
									<a
										href="#S4.SS3.p3"
										title="IV-C Code Generation and Execution ‣ IV Experiment Operation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§IV-C</span></a
									>,
									<a
										href="#S5.p1"
										title="V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§V</span></a
									>.
								</span>
							</li>
							<li id="bib.bib53" class="ltx_bibitem ltx_bib_article">
								<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[3]</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_author"
										>M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards,
										Y. Burda, N. Joseph, G. Brockman,
										<span class="ltx_text ltx_bib_etal">et al.</span></span
									><span class="ltx_text ltx_bib_year"> (2021)</span>
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_title"
										>Evaluating large language models trained on code</span
									>.
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2107.03374</span>.
								</span>
								<span class="ltx_bibblock ltx_bib_cited"
									>Cited by:
									<a
										href="#S1.p2"
										title="I Introduction ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§I</span></a
									>.
								</span>
							</li>
							<li id="bib.bib25" class="ltx_bibitem ltx_bib_inproceedings">
								<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[4]</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_author"
										>T. Coignion, C. Quinton, and R. Rouvoy</span
									><span class="ltx_text ltx_bib_year"> (2024)</span>
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_title"
										>A performance study of llm-generated code on leetcode</span
									>.
								</span>
								<span class="ltx_bibblock"
									>In
									<span class="ltx_text ltx_bib_inbook"
										>Proceedings of the 28th International Conference on Evaluation and Assessment
										in Software Engineering</span
									>,
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_pages"> pp. 79–89</span>.
								</span>
								<span class="ltx_bibblock ltx_bib_cited"
									>Cited by:
									<a
										href="#S1.p2"
										title="I Introduction ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§I</span></a
									>,
									<a
										href="#S2.p1"
										title="II Related Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§II</span></a
									>.
								</span>
							</li>
							<li id="bib.bib74" class="ltx_bibitem ltx_bib_inproceedings">
								<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[5]</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_author"
										>B. Grewal, W. Lu, S. Nadi, and C. Bezemer</span
									><span class="ltx_text ltx_bib_year"> (2024)</span>
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_title"
										>Analyzing developer use of chatgpt generated code in open source github
										projects</span
									>.
								</span>
								<span class="ltx_bibblock"
									>In
									<span class="ltx_text ltx_bib_inbook"
										>2024 IEEE/ACM 21st International Conference on Mining Software Repositories
										(MSR)</span
									>,
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_pages"> pp. 157–161</span>.
								</span>
								<span class="ltx_bibblock ltx_bib_cited"
									>Cited by:
									<a
										href="#S1.p2"
										title="I Introduction ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§I</span></a
									>.
								</span>
							</li>
							<li id="bib.bib73" class="ltx_bibitem ltx_bib_article">
								<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[6]</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_author"
										>X. Gu, M. Chen, Y. Lin, Y. Hu, H. Zhang, C. Wan, Z. Wei, Y. Xu, and J.
										Wang</span
									><span class="ltx_text ltx_bib_year"> (2024)</span>
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_title"
										>On the effectiveness of large language models in domain-specific code
										generation</span
									>.
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_journal"
										>ACM Transactions on Software Engineering and Methodology</span
									>.
								</span>
								<span class="ltx_bibblock ltx_bib_cited"
									>Cited by:
									<a
										href="#S1.p2"
										title="I Introduction ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§I</span></a
									>.
								</span>
							</li>
							<li id="bib.bib8" class="ltx_bibitem ltx_bib_article">
								<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[7]</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_author"
										>A. Halevy, Y. Choi, A. Floratou, M. J. Franklin, N. Noy, and H. Wang</span
									><span class="ltx_text ltx_bib_year"> (2023)</span>
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_title"
										>Will llms reshape, supercharge, or kill data science?(vldb 2023 panel)</span
									>.
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_journal">Proceedings of the VLDB Endowment</span>
									<span class="ltx_text ltx_bib_volume">16</span> (<span
										class="ltx_text ltx_bib_number"
										>12</span
									>), <span class="ltx_text ltx_bib_pages"> pp. 4114–4115</span>.
								</span>
								<span class="ltx_bibblock ltx_bib_cited"
									>Cited by:
									<a
										href="#S1.p1"
										title="I Introduction ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§I</span></a
									>.
								</span>
							</li>
							<li id="bib.bib69" class="ltx_bibitem ltx_bib_inproceedings">
								<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[8]</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_author"
										>M. Kazemitabaar, J. Williams, I. Drosos, T. Grossman, A. Z. Henley, C.
										Negreanu, and A. Sarkar</span
									><span class="ltx_text ltx_bib_year"> (2024)</span>
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_title"
										>Improving steering and verification in ai-assisted data analysis with
										interactive task decomposition</span
									>.
								</span>
								<span class="ltx_bibblock"
									>In
									<span class="ltx_text ltx_bib_inbook"
										>Proceedings of the 37th Annual ACM Symposium on User Interface Software and
										Technology</span
									>,
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_pages"> pp. 1–19</span>.
								</span>
								<span class="ltx_bibblock ltx_bib_cited"
									>Cited by:
									<a
										href="#S1.p1"
										title="I Introduction ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§I</span></a
									>,
									<a
										href="#S2.p2"
										title="II Related Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§II</span></a
									>.
								</span>
							</li>
							<li id="bib.bib26" class="ltx_bibitem ltx_bib_article">
								<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[9]</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_author"
										>M. A. Kuhail, S. S. Mathew, A. Khalil, J. Berengueres, and S. J. H. Shah</span
									><span class="ltx_text ltx_bib_year"> (2024)</span>
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_title"
										>“Will i be replaced?” assessing chatgpt’s effect on software development and
										programmer perceptions of ai tools</span
									>.
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_journal">Science of Computer Programming</span>
									<span class="ltx_text ltx_bib_volume">235</span>,
									<span class="ltx_text ltx_bib_pages"> pp. 103111</span>.
								</span>
								<span class="ltx_bibblock ltx_bib_cited"
									>Cited by:
									<a
										href="#S1.p2"
										title="I Introduction ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§I</span></a
									>,
									<a
										href="#S2.p1"
										title="II Related Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§II</span></a
									>.
								</span>
							</li>
							<li id="bib.bib72" class="ltx_bibitem ltx_bib_inproceedings">
								<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[10]</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_author"
										>Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W. Yih, D. Fried,
										S. Wang, and T. Yu</span
									><span class="ltx_text ltx_bib_year"> (2023)</span>
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_title"
										>DS-1000: a natural and reliable benchmark for data science code
										generation</span
									>.
								</span>
								<span class="ltx_bibblock"
									>In
									<span class="ltx_text ltx_bib_inbook"
										>International Conference on Machine Learning</span
									>,
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_pages"> pp. 18319–18345</span>.
								</span>
								<span class="ltx_bibblock ltx_bib_cited"
									>Cited by:
									<a
										href="#S1.p1"
										title="I Introduction ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§I</span></a
									>,
									<a
										href="#S2.p3"
										title="II Related Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§II</span></a
									>,
									<a
										href="#S8.SS2.p2"
										title="VIII-B Expanding Model Diversity and Dataset Coverage ‣ VIII Future Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§VIII-B</span></a
									>.
								</span>
							</li>
							<li id="bib.bib66" class="ltx_bibitem ltx_bib_article">
								<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[11]</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_author"
										>J. Li, B. Hui, G. Qu, J. Yang, B. Li, B. Li, B. Wang, B. Qin, R. Geng, N. Huo,
										<span class="ltx_text ltx_bib_etal">et al.</span></span
									><span class="ltx_text ltx_bib_year"> (2024)</span>
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_title"
										>Can llm already serve as a database interface? a big bench for large-scale
										database grounded text-to-sqls</span
									>.
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_journal"
										>Advances in Neural Information Processing Systems</span
									>
									<span class="ltx_text ltx_bib_volume">36</span>.
								</span>
								<span class="ltx_bibblock ltx_bib_cited"
									>Cited by:
									<a
										href="#S1.p1"
										title="I Introduction ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§I</span></a
									>,
									<a
										href="#S2.p2"
										title="II Related Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§II</span></a
									>.
								</span>
							</li>
							<li id="bib.bib27" class="ltx_bibitem ltx_bib_inproceedings">
								<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[12]</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_author"
										>M. Malekpour, N. Shaheen, F. Khomh, and A. Mhedhbi</span
									>
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_title"
										>Towards optimizing sql generation via llm routing</span
									>.
								</span>
								<span class="ltx_bibblock"
									>In
									<span class="ltx_text ltx_bib_inbook"
										>NeurIPS 2024 Third Table Representation Learning Workshop</span
									>,
								</span>
								<span class="ltx_bibblock ltx_bib_cited"
									>Cited by:
									<a
										href="#S2.p2"
										title="II Related Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§II</span></a
									>.
								</span>
							</li>
							<li id="bib.bib23" class="ltx_bibitem ltx_bib_inproceedings">
								<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[13]</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_author"
										>N. Nascimento, P. Alencar, and D. Cowan</span
									><span class="ltx_text ltx_bib_year"> (2023)</span>
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_title"
										>GPT-in-the-loop: supporting adaptation in multiagent systems</span
									>.
								</span>
								<span class="ltx_bibblock"
									>In
									<span class="ltx_text ltx_bib_inbook"
										>2023 IEEE International Conference on Big Data (BigData)</span
									>,
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_pages"> pp. 4674–4683</span>.
								</span>
								<span class="ltx_bibblock ltx_bib_cited"
									>Cited by:
									<a
										href="#S8.SS1.p1"
										title="VIII-A Exploring Complex and Real-World Data Science Tasks ‣ VIII Future Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§VIII-A</span></a
									>.
								</span>
							</li>
							<li id="bib.bib65" class="ltx_bibitem ltx_bib_inproceedings">
								<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[14]</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_author"
										>N. Nascimento, C. Tavares, P. Alencar, and D. Cowan</span
									><span class="ltx_text ltx_bib_year"> (2023)</span>
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_title"
										>GPT in data science: a practical exploration of model selection</span
									>.
								</span>
								<span class="ltx_bibblock"
									>In
									<span class="ltx_text ltx_bib_inbook"
										>2023 IEEE International Conference on Big Data (BigData)</span
									>,
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_pages"> pp. 4325–4334</span>.
								</span>
								<span class="ltx_bibblock ltx_bib_cited"
									>Cited by:
									<a
										href="#S1.p1"
										title="I Introduction ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§I</span></a
									>.
								</span>
							</li>
							<li id="bib.bib21" class="ltx_bibitem ltx_bib_inproceedings">
								<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[15]</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_author">N. Nathalia, A. Paulo, and C. Donald</span
									><span class="ltx_text ltx_bib_year"> (2023)</span>
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_title"
										>Artificial intelligence vs. software engineers: an empirical study on
										performance and efficiency using chatgpt</span
									>.
								</span>
								<span class="ltx_bibblock"
									>In
									<span class="ltx_text ltx_bib_inbook"
										>Proceedings of the 33rd Annual International Conference on Computer Science and
										Software Engineering</span
									>,
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_pages"> pp. 24–33</span>.
								</span>
								<span class="ltx_bibblock ltx_bib_cited"
									>Cited by:
									<a
										href="#S1.p2"
										title="I Introduction ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§I</span></a
									>,
									<a
										href="#S2.p1"
										title="II Related Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§II</span></a
									>.
								</span>
							</li>
							<li id="bib.bib51" class="ltx_bibitem ltx_bib_inproceedings">
								<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[16]</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_author">N. Nguyen and S. Nadi</span
									><span class="ltx_text ltx_bib_year"> (2022)</span>
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_title"
										>An empirical evaluation of github copilot’s code suggestions</span
									>.
								</span>
								<span class="ltx_bibblock"
									>In
									<span class="ltx_text ltx_bib_inbook"
										>Proceedings of the 19th International Conference on Mining Software
										Repositories</span
									>,
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_pages"> pp. 1–5</span>.
								</span>
								<span class="ltx_bibblock ltx_bib_cited"
									>Cited by:
									<a
										href="#S1.p2"
										title="I Introduction ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§I</span></a
									>,
									<a
										href="#S2.p1"
										title="II Related Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§II</span></a
									>.
								</span>
							</li>
							<li id="bib.bib77" class="ltx_bibitem ltx_bib_misc">
								<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[17]</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_author">Ritchie Vink</span
									><span class="ltx_text ltx_bib_year"> (2023)</span>
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_title"
										>Polars: blazingly fast dataframes in rust, python, node.js, r, and sql</span
									>.
								</span>
								<span class="ltx_bibblock"
									>External Links:
									<span class="ltx_text ltx_bib_links"
										><a
											href="https://github.com/pola-rs/polars"
											title=""
											class="ltx_ref ltx_bib_external"
											>Link</a
										></span
									>
								</span>
								<span class="ltx_bibblock ltx_bib_cited"
									>Cited by:
									<a
										href="#S8.SS2.p2"
										title="VIII-B Expanding Model Diversity and Dataset Coverage ‣ VIII Future Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§VIII-B</span></a
									>.
								</span>
							</li>
							<li id="bib.bib70" class="ltx_bibitem ltx_bib_misc">
								<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[18]</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_author">StrataScratch</span
									><span class="ltx_text ltx_bib_year"> (n.d.)</span>
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_title">Master coding for data science</span>.
								</span>
								<span class="ltx_bibblock"
									>Note:
									<span class="ltx_text ltx_bib_note"
										><span class="ltx_ERROR undefined">\url</span
										>https://www.stratascratch.com/Accessed: 2024-11-01</span
									>
								</span>
								<span class="ltx_bibblock ltx_bib_cited"
									>Cited by:
									<a
										href="#S1.p3"
										title="I Introduction ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§I</span></a
									>,
									<a
										href="#S3.p1"
										title="III Controlled Experiment ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§III</span></a
									>,
									<a
										href="#S4.I1.i1.p1"
										title="In IV Experiment Operation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">item 1</span></a
									>,
									<a
										href="#S4.SS1.p1"
										title="IV-A Dataset: Selection of Data Science Problems ‣ IV Experiment Operation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§IV-A</span></a
									>.
								</span>
							</li>
							<li id="bib.bib12" class="ltx_bibitem ltx_bib_article">
								<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[19]</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_author"
										>C. Troy, S. Sturley, J. M. Alcaraz-Calero, and Q. Wang</span
									><span class="ltx_text ltx_bib_year"> (2023)</span>
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_title"
										>Enabling generative ai to produce sql statements: a framework for the
										auto-generation of knowledge based on ebnf context-free grammars</span
									>.
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_journal">IEEE Access</span>
									<span class="ltx_text ltx_bib_volume">11</span>,
									<span class="ltx_text ltx_bib_pages"> pp. 123543–123564</span>.
								</span>
								<span class="ltx_bibblock ltx_bib_cited"
									>Cited by:
									<a
										href="#S2.p2"
										title="II Related Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§II</span></a
									>.
								</span>
							</li>
							<li id="bib.bib36" class="ltx_bibitem ltx_bib_article">
								<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[20]</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_author"
										>J. White, S. Hays, Q. Fu, J. Spencer-Smith, and D. C. Schmidt</span
									><span class="ltx_text ltx_bib_year"> (2023)</span>
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_title"
										>ChatGPT prompt patterns for improving code quality, refactoring, requirements
										elicitation, and software design</span
									>.
								</span>
								<span class="ltx_bibblock"
									>External Links:
									<span class="ltx_text ltx_bib_links"
										><a
											href="https://dx.doi.org/https%3A//doi.org/10.48550/arXiv.2303.07839"
											title=""
											class="ltx_ref doi ltx_bib_external"
											>Document</a
										></span
									>
								</span>
								<span class="ltx_bibblock ltx_bib_cited"
									>Cited by:
									<a
										href="#S5.SS6.SSS1.p2"
										title="V-F1 Internal Validity ‣ V-F Threats to Validity ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§V-F1</span></a
									>.
								</span>
							</li>
							<li id="bib.bib24" class="ltx_bibitem ltx_bib_book">
								<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[21]</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_author"
										>C. Wohlin, P. Runeson, M. Höst, M. C. Ohlsson, B. Regnell, A. Wesslén,
										<span class="ltx_text ltx_bib_etal">et al.</span></span
									><span class="ltx_text ltx_bib_year"> (2012)</span>
								</span>
								<span class="ltx_bibblock"
									><span class="ltx_text ltx_bib_title"
										>Experimentation in software engineering</span
									>.
								</span>
								<span class="ltx_bibblock"
									>Vol. <span class="ltx_text ltx_bib_volume">236</span>,
									<span class="ltx_text ltx_bib_publisher">Springer</span>.
								</span>
								<span class="ltx_bibblock ltx_bib_cited"
									>Cited by:
									<a
										href="#S1.p3"
										title="I Introduction ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§I</span></a
									>,
									<a
										href="#S3.p1"
										title="III Controlled Experiment ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§III</span></a
									>,
									<a
										href="#S5.SS1.p2"
										title="V-A RQ1: Success Rate of LLMs in Solving Data Science Problems ‣ V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation"
										class="ltx_ref"
										><span class="ltx_text ltx_ref_tag">§V-A</span></a
									>.
								</span>
							</li>
						</ul>
					</section>
				</article>
			</div>
			<footer class="ltx_page_footer">
				<div class="ltx_page_logo">
					Generated by
					<a href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"
						><span style="letter-spacing: -0.2em; margin-right: 0.1em"
							>L<span class="ltx_font_smallcaps" style="position: relative; bottom: 2.2pt">a</span
							>T<span
								class="ltx_font_smallcaps"
								style="font-size: 120%; position: relative; bottom: -0.2ex"
								>e</span
							></span
						><span style="font-size: 90%; position: relative; bottom: -0.2ex">XML</span
						><img
							src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="
							alt="Mascot Sammy"
					/></a>
				</div>
			</footer>
		</div>
	</body>
</html>
