<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>III Controlled Experiment ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation</title>
<!--Generated by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->

<link rel="stylesheet" href="LaTeXML.css" type="text/css">
<link rel="stylesheet" href="ltx-article.css" type="text/css">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js?config=MML_HTMLorMML"></script>
<link rel="up" href="paper.html" title="LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="start" href="paper.html" title="LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="prev" href="S2.html" title="II Related Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="next" href="S4.html" title="IV Experiment Operation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S1.html" title="I Introduction ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S2.html" title="II Related Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S4.html" title="IV Experiment Operation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S5.html" title="V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S6.html" title="VI Discussion ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S7.html" title="VII Conclusion ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S8.html" title="VIII Future Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="bibliography" href="bib.html" title="References ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
</head>
<body>
<nav class="ltx_page_navbar"><a href="paper.html" title="" class="ltx_ref" rel="start"><span class="ltx_text ltx_ref_title">LLM4DS: Evaluating Large Language Models for Data Science Code Generation</span></a>
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a href="S1.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S2.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section ltx_ref_self">
<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Controlled Experiment</span></span></span>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#SS1" title="In III Controlled Experiment ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III-A </span><span class="ltx_text ltx_font_italic">Research Questions, Hypotheses, and Metrics</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#SS2" title="In III Controlled Experiment ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III-B </span><span class="ltx_text ltx_font_italic">Variables Selection</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S4.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiment Operation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S5.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Analysis and Interpretation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S6.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S7.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S8.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII </span><span class="ltx_text ltx_font_smallcaps">Future Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_bibliography"><a href="bib.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title">References</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<header class="ltx_page_header">
<div class="ltx_align_center">
<a href="paper.html" title="" class="ltx_ref" rel="up"><span class="ltx_text ltx_ref_title">LLM4DS: Evaluating Large Language Models for Data Science Code Generation</span></a><a href="S2.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref" rel="prev"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a><a href="S4.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref" rel="next"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiment Operation</span></span></a>
</div></header>
<div class="ltx_page_content">
<section class="ltx_section ltx_authors_1line">
<h1 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps">Controlled Experiment</span>
</h1>

<div id="p1" class="ltx_para">
<p class="ltx_p">In line with the controlled experiment methodology by Wohlin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="bib.html#bib24" title="Experimentation in software engineering" class="ltx_ref">21</a>]</cite>, our study aims to evaluate and compare the effectiveness of four prominent LLM-based AI assistants—Microsoft Copilot (GPT-4 Turbo), ChatGPT (o1-preview), Claude (3.5 Sonnet), and Perplexity Lab (Llama-3.1-70b-instruct)—in solving data science coding tasks sourced from the Stratascratch platform <cite class="ltx_cite ltx_citemacro_cite">[<a href="bib.html#bib70" title="Master coding for data science" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="p2" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Effectiveness</span> in this context refers to the degree to which these models achieve desired outcomes across four key aspects: <span class="ltx_text ltx_font_italic">success rate</span>, <span class="ltx_text ltx_font_italic">efficiency</span>, <span class="ltx_text ltx_font_italic">quality of output</span>, and <span class="ltx_text ltx_font_italic">consistency</span>. Specifically, we define:</p>
<ul id="I1" class="ltx_itemize">
<li id="I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="I1.i1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Success Rate</span> as the proportion of correctly generated code solutions, measured by the percentage of solutions that achieve the correct result regardless of the number of attempts;</p>
</div>
</li>
<li id="I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="I1.i2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Efficiency</span> as the runtime execution speed of the generated solution;</p>
</div>
</li>
<li id="I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="I1.i3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Quality of Output</span> as the alignment of generated solutions with expected outcomes, particularly for visualization tasks;</p>
</div>
</li>
<li id="I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="I1.i4.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Consistency</span> as the reliability of each model’s performance across varying difficulty levels and task types.</p>
</div>
</li>
</ul>
</div>
<section id="SS1" class="ltx_subsection">
<h2 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III-A </span><span class="ltx_text ltx_font_italic">Research Questions, Hypotheses, and Metrics</span>
</h2>

<div id="SS1.p1" class="ltx_para">
<p class="ltx_p">To systematically explore effectiveness, we structured our investigation around specific research questions, each accompanied by testable hypotheses and relevant evaluation metrics. Table <a href="#T1" title="TABLE I ‣ III-A Research Questions, Hypotheses, and Metrics ‣ III Controlled Experiment ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> details these research questions, hypotheses, and corresponding metrics.</p>
</div>
<figure id="T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Research Questions, Hypotheses, and Metrics</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:148.0pt;"><span class="ltx_text ltx_font_bold">Research Question</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:105.3pt;"><span class="ltx_text ltx_font_bold">Null Hypothesis</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:105.3pt;"><span class="ltx_text ltx_font_bold">Alternative Hypothesis</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:105.3pt;"><span class="ltx_text ltx_font_bold">Metrics</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:148.0pt;">RQ1: How successful are LLMs in solving data science coding problems, and do they outperform each other in success rate?</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:105.3pt;">H0_1: The success rate of each LLM in solving data science coding problems is not significantly higher than random chance (50%). 
<br class="ltx_break">H0_1a: There is no significant difference in success rates between LLM pairs.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:105.3pt;">H1_1: The success rate of each LLM in solving data science coding problems is significantly higher than random chance (50%). 
<br class="ltx_break">H1_1a: At least one pair of LLMs shows a significant difference in success rates.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:105.3pt;">Overall success rate (percentage of correct solutions) and pairwise success rate comparisons</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:148.0pt;">RQ2: Does the difficulty level of coding problems (easy, medium, hard) influence the success rate of the different LLMs, and do specific LLMs outperform others at each difficulty level?</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:105.3pt;">H0_2: Difficulty level does not significantly affect the success rate of the LLMs. 
<br class="ltx_break">H0_2a: There is no significant difference in success rates between LLM pairs within each difficulty level.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:105.3pt;">H1_2: The success rate of the LLMs varies significantly with difficulty level. 
<br class="ltx_break">H1_2a: At least one pair of LLMs shows a significant difference in success rate within a specific difficulty level.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:105.3pt;">Success rate (percentage of correct solutions) across difficulty levels and pairwise success rate comparisons within each level</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:148.0pt;">RQ3: Does the type of data science task (Analytical, Algorithm, Visualization) influence the success rate of the different LLMs, and do specific LLMs outperform others for certain task types?</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:105.3pt;">H0_3: The type of data science task does not significantly impact the LLMs’ success rate. 
<br class="ltx_break">H0_3a: There is no significant difference in success rates between LLM pairs within each task type.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:105.3pt;">H1_3: The success rate of the LLMs varies significantly with the type of data science task. 
<br class="ltx_break">H1_3a: At least one pair of LLMs shows a significant difference in success rate within a specific task type.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:105.3pt;">Success rate (percentage of correct solutions) for each task type and pairwise success rate comparisons within each type</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:148.0pt;">RQ4: For Analytical questions, do the LLMs differ in the efficiency (execution time) of the code they generate?</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:105.3pt;">H0_4: The population medians of the execution times across the LLMs for Analytical questions are equal.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:105.3pt;">H1_4: At least one LLM has a different population median execution time for Analytical questions compared to others.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:105.3pt;">Execution time for each generated solution on Analytical problems, per LLM</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:148.0pt;">RQ5: For visualization tasks, do the LLMs differ in the quality (similarity) of the visual outputs they produce compared to expected results?</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:105.3pt;">H0_5: The population medians of the similarity scores for visualization outputs across the LLMs are equal.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:105.3pt;">H1_5: At least one LLM has a different population median similarity score for visualization outputs compared to others.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:105.3pt;">Similarity scores for visualization outputs compared to expected results</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="SS2" class="ltx_subsection">
<h2 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III-B </span><span class="ltx_text ltx_font_italic">Variables Selection</span>
</h2>

<div id="SS2.p1" class="ltx_para">
<p class="ltx_p">To structure our analysis, we identified key variables that allow us to examine the performance of each AI assistant across different problem types and difficulty levels.</p>
</div>
<div id="SS2.p2" class="ltx_para">
<p class="ltx_p">The <span class="ltx_text ltx_font_bold">independent variables</span> in this study, which we controlled or varied, include:</p>
<ul id="I2" class="ltx_itemize">
<li id="I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="I2.i1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">LLM-based AI assistants</span>: The four AI models under evaluation—Microsoft Copilot, ChatGPT, Claude, and Perplexity Lab.</p>
</div>
</li>
<li id="I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="I2.i2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Difficulty level of coding problems</span>: Easy, Medium, Hard.</p>
</div>
</li>
<li id="I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="I2.i3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Type of Data Science task</span>: Analytical, Algorithm, Visualization.</p>
</div>
</li>
</ul>
</div>
<div id="SS2.p3" class="ltx_para">
<p class="ltx_p">The <span class="ltx_text ltx_font_bold">dependent variables</span> are the metrics we measured to assess each AI assistant’s effectiveness:</p>
<ul id="I3" class="ltx_itemize">
<li id="I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="I3.i1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Success rate</span>: The percentage of correct solutions generated by each LLM, regardless of the number of attempts.</p>
</div>
</li>
<li id="I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="I3.i2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Running time</span>: Execution time of code for Analytical questions.</p>
</div>
</li>
<li id="I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="I3.i3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Graph similarity scores</span>: Similarity between generated and expected graphs for Visualization questions.</p>
</div>
</li>
</ul>
</div>
<div id="SS2.p4" class="ltx_para">
<p class="ltx_p">These variables connect directly to the research questions and metrics outlined in Table <a href="#T1" title="TABLE I ‣ III-A Research Questions, Hypotheses, and Metrics ‣ III Controlled Experiment ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, allowing us to systematically investigate the impact of each independent variable on the AI models’ performance.</p>
</div>
</section>
</section>
</div>
<footer class="ltx_page_footer">
<div class="ltx_align_center">
<a href="S2.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref" rel="prev"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a><a href="bib.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref" rel="bibliography"><span class="ltx_text ltx_ref_title">References</span></a><a href="S4.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref" rel="next"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiment Operation</span></span></a>
</div>
<div class="ltx_page_logo">Generated  by <a href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>
</body>
</html>
