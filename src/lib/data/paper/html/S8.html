<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>VIII Future Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation</title>
<!--Generated by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->

<link rel="stylesheet" href="LaTeXML.css" type="text/css">
<link rel="stylesheet" href="ltx-article.css" type="text/css">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js?config=MML_HTMLorMML"></script>
<link rel="up" href="paper.html" title="LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="start" href="paper.html" title="LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="prev" href="S7.html" title="VII Conclusion ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="next" href="bib.html" title="References ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S1.html" title="I Introduction ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S2.html" title="II Related Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S3.html" title="III Controlled Experiment ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S4.html" title="IV Experiment Operation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S5.html" title="V Analysis and Interpretation ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S6.html" title="VI Discussion ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="section" href="S7.html" title="VII Conclusion ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
<link rel="bibliography" href="bib.html" title="References ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation">
</head>
<body>
<nav class="ltx_page_navbar"><a href="paper.html" title="" class="ltx_ref" rel="start"><span class="ltx_text ltx_ref_title">LLM4DS: Evaluating Large Language Models for Data Science Code Generation</span></a>
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a href="S1.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S2.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S3.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Controlled Experiment</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S4.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiment Operation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S5.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Analysis and Interpretation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S6.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="S7.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section ltx_ref_self">
<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII </span><span class="ltx_text ltx_font_smallcaps">Future Work</span></span></span>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#SS1" title="In VIII Future Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII-A </span><span class="ltx_text ltx_font_italic">Exploring Complex and Real-World Data Science Tasks</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#SS2" title="In VIII Future Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII-B </span><span class="ltx_text ltx_font_italic">Expanding Model Diversity and Dataset Coverage</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#SS3" title="In VIII Future Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII-C </span><span class="ltx_text ltx_font_italic">Expanding Evaluation Metrics</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#SS4" title="In VIII Future Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII-D </span><span class="ltx_text ltx_font_italic">Investigating Prompt Engineering and Ensuring Reproducibility</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#SS5" title="In VIII Future Work ‣ LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII-E </span><span class="ltx_text ltx_font_italic">Exploring Further Research Questions</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_bibliography"><a href="bib.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_title">References</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<header class="ltx_page_header">
<div class="ltx_align_center">
<a href="paper.html" title="" class="ltx_ref" rel="up"><span class="ltx_text ltx_ref_title">LLM4DS: Evaluating Large Language Models for Data Science Code Generation</span></a><a href="S7.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref" rel="prev"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a><a href="bib.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref" rel="next"><span class="ltx_text ltx_ref_title">References</span></a>
</div></header>
<div class="ltx_page_content">
<section class="ltx_section ltx_authors_1line">
<h1 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span class="ltx_text ltx_font_smallcaps">Future Work</span>
</h1>

<div id="p1" class="ltx_para">
<p class="ltx_p">Our study opens several avenues for future research to enhance the application of LLMs in data science code generation.</p>
</div>
<section id="SS1" class="ltx_subsection">
<h2 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">VIII-A </span><span class="ltx_text ltx_font_italic">Exploring Complex and Real-World Data Science Tasks</span>
</h2>

<div id="SS1.p1" class="ltx_para">
<p class="ltx_p">Evaluating LLMs on sophisticated, real-world data science tasks—such as implementing machine learning models with libraries like Scikit-learn or TensorFlow, handling large datasets, and working with unstructured data—could provide deeper insights into their capabilities and limitations. For instance, Nascimento et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="bib.html#bib23" title="GPT-in-the-loop: supporting adaptation in multiagent systems" class="ltx_ref">13</a>]</cite> demonstrated the use of an LLM to replace a learning algorithm that involved neural networks optimized through genetic algorithms. While their experiment was preliminary, it highlighted the potential of LLMs to automate complex coding solutions, suggesting that these models could extend beyond basic scripting to more advanced tasks. Exploring tasks like multivariate analysis, time series forecasting, and dynamic optimization could further test LLM proficiency. Testing in practical settings uncovers challenges that controlled experiments may not fully capture.</p>
</div>
</section>
<section id="SS2" class="ltx_subsection">
<h2 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">VIII-B </span><span class="ltx_text ltx_font_italic">Expanding Model Diversity and Dataset Coverage</span>
</h2>

<div id="SS2.p1" class="ltx_para">
<p class="ltx_p">We could extend this analysis by integrating additional LLMs and incorporating data science-specific coding challenges from various platforms, such as LeetCode, with tasks like data manipulation, cleaning, and SQL queries. To capture a broader range of data science skills, the dataset could also include non-coding tasks, such as interpretation and analysis questions, as provided by Stratascratch.</p>
</div>
<div id="SS2.p2" class="ltx_para">
<p class="ltx_p">Additionally, we could integrate recently released questions in Stratascratch that use Polars DataFrame <cite class="ltx_cite ltx_citemacro_cite">[<a href="bib.html#bib77" title="Polars: blazingly fast dataframes in rust, python, node.js, r, and sql" class="ltx_ref">17</a>]</cite> for data manipulation-a high-performance library designed for efficient data handling in Python.
We could further expand the dataset by incorporating problems from DS-1000 <cite class="ltx_cite ltx_citemacro_cite">[<a href="bib.html#bib72" title="DS-1000: a natural and reliable benchmark for data science code generation" class="ltx_ref">10</a>]</cite>, which includes a diverse selection of data science problems sourced from StackOverflow. Following recommendations by Lai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="bib.html#bib72" title="DS-1000: a natural and reliable benchmark for data science code generation" class="ltx_ref">10</a>]</cite>, introducing customized variations of existing problems would help reduce model memorization, enhancing the rigor of the evaluation environment.</p>
</div>
</section>
<section id="SS3" class="ltx_subsection">
<h2 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">VIII-C </span><span class="ltx_text ltx_font_italic">Expanding Evaluation Metrics</span>
</h2>

<div id="SS3.p1" class="ltx_para">
<p class="ltx_p">Future work could expand LLM evaluation by integrating software engineering metrics like code complexity, maintainability, and readability. Code similarity analysis could assess alignment with industry standards, while qualitative reviews by data scientists would add valuable insights, particularly for visualization tasks where image similarity metrics may fall short.</p>
</div>
</section>
<section id="SS4" class="ltx_subsection">
<h2 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">VIII-D </span><span class="ltx_text ltx_font_italic">Investigating Prompt Engineering and Ensuring Reproducibility</span>
</h2>

<div id="SS4.p1" class="ltx_para">
<p class="ltx_p">Prompt engineering significantly influences LLM outputs. Future research should examine how different prompt formulations affect code generation quality and consistency. Employing methodologies where LLMs simulate multiple users <cite class="ltx_cite ltx_citemacro_cite">[<a href="bib.html#bib15" title="Using large language models to simulate multiple humans and replicate human subject studies" class="ltx_ref">1</a>]</cite> could shed light on the impact of varying professional experiences and prompt designs. Addressing the non-deterministic nature of LLMs by controlling parameters like temperature settings could improve reproducibility, leading to more consistent and reliable evaluations.</p>
</div>
</section>
<section id="SS5" class="ltx_subsection">
<h2 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">VIII-E </span><span class="ltx_text ltx_font_italic">Exploring Further Research Questions</span>
</h2>

<div id="SS5.p1" class="ltx_para">
<p class="ltx_p">Even using the same dataset, many additional questions and hypotheses remain to be explored. For instance, while we assessed the impact of problem difficulty and type on LLM success rates, further analysis could focus on establishing baseline success rates for each problem type and difficulty level. Given the general success baseline of 60%, future research might explore optimal baseline thresholds specific to each type and level of task.
Beyond success rates, this dataset also allows for an in-depth exploration of efficiency (execution times) and similarity scores for each difficulty level, providing a more comprehensive view of model performance in diverse task complexity. Additionally, information on the number of attempts (up to three) and instances of minor edits provides data for assessing error types (syntax and logic errors), retry patterns, and the models’ adaptability to user feedback. Our dataset also includes specific topics within each question type, allowing for a more granular analysis that could reveal topic-specific strengths or limitations of each LLM.</p>
</div>
</section>
</section>
</div>
<footer class="ltx_page_footer">
<div class="ltx_align_center">
<a href="S7.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref" rel="prev"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a><a href="bib.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref" rel="bibliography"><span class="ltx_text ltx_ref_title">References</span></a><a href="bib.html" title="In LLM4DS: Evaluating Large Language Models for Data Science Code Generation" class="ltx_ref" rel="next"><span class="ltx_text ltx_ref_title">References</span></a>
</div>
<div class="ltx_page_logo">Generated  by <a href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>
</body>
</html>
